{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Import Dependencies\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea66e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Database Configuration\n",
    "# ============================================================================\n",
    "\n",
    "db_config = {\n",
    "    'host': '127.0.0.1',\n",
    "    'user': 'root',\n",
    "    'password': '',\n",
    "    'database': 'trading_system'\n",
    "}\n",
    "\n",
    "print(\"✓ Database configuration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d65143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Ensemble Model Class\n",
    "# ============================================================================\n",
    "\n",
    "class EnsembleModels:\n",
    "    def __init__(self, db_config, model_save_dir='models/ensemble_models'):\n",
    "        self.db_config = db_config\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.setup_logging()\n",
    "        self.create_model_directory()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('ensemble_models.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def create_model_directory(self):\n",
    "        if not os.path.exists(self.model_save_dir):\n",
    "            os.makedirs(self.model_save_dir)\n",
    "            self.logger.info(f\"Created ensemble model directory: {self.model_save_dir}\")\n",
    "    \n",
    "    def connect_db(self):\n",
    "        try:\n",
    "            conn = mysql.connector.connect(**self.db_config)\n",
    "            return conn\n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Database connection error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate performance metrics\"\"\"\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        try:\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "        except:\n",
    "            r2 = -999\n",
    "        \n",
    "        # Directional accuracy\n",
    "        true_direction = y_true > 0\n",
    "        pred_direction = y_pred > 0\n",
    "        dir_acc = (np.sum(true_direction == pred_direction) / len(true_direction)) * 100\n",
    "        \n",
    "        return {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2_score': r2,\n",
    "            'directional_accuracy': dir_acc\n",
    "        }\n",
    "    \n",
    "    def load_model_predictions(self, symbol):\n",
    "        \"\"\"Load predictions from all trained models for a stock\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return None\n",
    "        \n",
    "        # Get all model predictions from database\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            model_type,\n",
    "            rmse,\n",
    "            mae,\n",
    "            directional_accuracy\n",
    "        FROM model_performance\n",
    "        WHERE symbol = %s\n",
    "        ORDER BY directional_accuracy DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = conn.cursor(dictionary=True)\n",
    "        cursor.execute(query, (symbol,))\n",
    "        model_info = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        if not model_info:\n",
    "            self.logger.warning(f\"No model performance data found for {symbol}\")\n",
    "            return None\n",
    "        \n",
    "        return model_info\n",
    "    \n",
    "    def simple_average_ensemble(self, predictions_dict):\n",
    "        \"\"\"\n",
    "        Method 1: Simple Average Ensemble\n",
    "        Average predictions from all models equally\n",
    "        \"\"\"\n",
    "        predictions = list(predictions_dict.values())\n",
    "        ensemble_pred = np.mean(predictions, axis=0)\n",
    "        return ensemble_pred\n",
    "    \n",
    "    def weighted_average_ensemble(self, predictions_dict, weights_dict):\n",
    "        \"\"\"\n",
    "        Method 2: Weighted Average Ensemble\n",
    "        Weight predictions by directional accuracy\n",
    "        \"\"\"\n",
    "        ensemble_pred = np.zeros_like(list(predictions_dict.values())[0])\n",
    "        total_weight = sum(weights_dict.values())\n",
    "        \n",
    "        for model_name, predictions in predictions_dict.items():\n",
    "            weight = weights_dict.get(model_name, 0)\n",
    "            ensemble_pred += (predictions * weight)\n",
    "        \n",
    "        ensemble_pred /= total_weight\n",
    "        return ensemble_pred\n",
    "    \n",
    "    def top_k_ensemble(self, predictions_dict, weights_dict, k=3):\n",
    "        \"\"\"\n",
    "        Method 3: Top-K Ensemble\n",
    "        Use only top K best performing models\n",
    "        \"\"\"\n",
    "        # Sort models by weight (directional accuracy)\n",
    "        sorted_models = sorted(weights_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k_models = dict(sorted_models[:k])\n",
    "        \n",
    "        # Average predictions from top K models\n",
    "        ensemble_pred = np.zeros_like(list(predictions_dict.values())[0])\n",
    "        total_weight = sum(top_k_models.values())\n",
    "        \n",
    "        for model_name, weight in top_k_models.items():\n",
    "            if model_name in predictions_dict:\n",
    "                ensemble_pred += (predictions_dict[model_name] * weight)\n",
    "        \n",
    "        ensemble_pred /= total_weight\n",
    "        return ensemble_pred\n",
    "    \n",
    "    def median_ensemble(self, predictions_dict):\n",
    "        \"\"\"\n",
    "        Method 4: Median Ensemble\n",
    "        Use median of all predictions (robust to outliers)\n",
    "        \"\"\"\n",
    "        predictions = np.array(list(predictions_dict.values()))\n",
    "        ensemble_pred = np.median(predictions, axis=0)\n",
    "        return ensemble_pred\n",
    "    \n",
    "    def create_ensemble_predictions(self, symbol, enhanced_results):\n",
    "        \"\"\"\n",
    "        Create ensemble predictions using all four methods\n",
    "        enhanced_results: dict with model results from enhanced training\n",
    "        \"\"\"\n",
    "        if not enhanced_results:\n",
    "            self.logger.warning(f\"No enhanced results for {symbol}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract predictions and weights\n",
    "        predictions_dict = {}\n",
    "        weights_dict = {}\n",
    "        \n",
    "        for model_name, result_data in enhanced_results.items():\n",
    "            predictions_dict[model_name] = result_data['predictions']\n",
    "            weights_dict[model_name] = result_data['metrics']['directional_accuracy']\n",
    "        \n",
    "        # Get y_test (same for all models)\n",
    "        y_test = list(enhanced_results.values())[0].get('y_test')\n",
    "        if y_test is None:\n",
    "            self.logger.error(f\"No y_test data found for {symbol}\")\n",
    "            return None\n",
    "        \n",
    "        ensemble_results = {}\n",
    "        \n",
    "        # 1. Simple Average Ensemble\n",
    "        simple_pred = self.simple_average_ensemble(predictions_dict)\n",
    "        simple_metrics = self.calculate_metrics(y_test, simple_pred)\n",
    "        ensemble_results['Simple_Average'] = {\n",
    "            'predictions': simple_pred,\n",
    "            'metrics': simple_metrics\n",
    "        }\n",
    "        \n",
    "        # 2. Weighted Average Ensemble\n",
    "        weighted_pred = self.weighted_average_ensemble(predictions_dict, weights_dict)\n",
    "        weighted_metrics = self.calculate_metrics(y_test, weighted_pred)\n",
    "        ensemble_results['Weighted_Average'] = {\n",
    "            'predictions': weighted_pred,\n",
    "            'metrics': weighted_metrics\n",
    "        }\n",
    "        \n",
    "        # 3. Top-3 Ensemble\n",
    "        top3_pred = self.top_k_ensemble(predictions_dict, weights_dict, k=3)\n",
    "        top3_metrics = self.calculate_metrics(y_test, top3_pred)\n",
    "        ensemble_results['Top3_Models'] = {\n",
    "            'predictions': top3_pred,\n",
    "            'metrics': top3_metrics\n",
    "        }\n",
    "        \n",
    "        # 4. Median Ensemble\n",
    "        median_pred = self.median_ensemble(predictions_dict)\n",
    "        median_metrics = self.calculate_metrics(y_test, median_pred)\n",
    "        ensemble_results['Median'] = {\n",
    "            'predictions': median_pred,\n",
    "            'metrics': median_metrics\n",
    "        }\n",
    "        \n",
    "        return ensemble_results\n",
    "    \n",
    "    def save_ensemble_performance(self, symbol, ensemble_type, metrics, \n",
    "                                  train_samples, test_samples, model_list, has_sentiment):\n",
    "        \"\"\"Save ensemble performance to database\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        model_type_label = f\"Ensemble_{ensemble_type}_{'Sentiment' if has_sentiment else 'NoSentiment'}\"\n",
    "        model_path = f\"{self.model_save_dir}/{symbol}_{ensemble_type}.pkl\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        INSERT INTO model_performance \n",
    "        (symbol, model_type, rmse, mae, r2_score, directional_accuracy,\n",
    "         train_samples, test_samples, feature_count, model_path)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        values = (\n",
    "            symbol, model_type_label,\n",
    "            metrics['rmse'], metrics['mae'], metrics['r2_score'],\n",
    "            metrics['directional_accuracy'],\n",
    "            train_samples, test_samples, 0, model_path\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            cursor.execute(query, values)\n",
    "            conn.commit()\n",
    "            return True\n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Error saving ensemble performance: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "    \n",
    "    def save_ensemble_data(self, symbol, ensemble_results):\n",
    "        \"\"\"Save ensemble predictions to file\"\"\"\n",
    "        filename = f\"{symbol}_ensemble.pkl\"\n",
    "        filepath = os.path.join(self.model_save_dir, filename)\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(ensemble_results, f)\n",
    "        \n",
    "        return filepath\n",
    "    \n",
    "    def train_all_ensembles(self, symbol, enhanced_results, train_samples, \n",
    "                           test_samples, has_sentiment):\n",
    "        \"\"\"Train all four ensemble methods for a stock\"\"\"\n",
    "        self.logger.info(f\"Creating ensemble models for {symbol}...\")\n",
    "        \n",
    "        ensemble_results = self.create_ensemble_predictions(symbol, enhanced_results)\n",
    "        \n",
    "        if not ensemble_results:\n",
    "            return None\n",
    "        \n",
    "        # Get model list\n",
    "        model_list = list(enhanced_results.keys())\n",
    "        \n",
    "        # Save performance for each ensemble method\n",
    "        for ensemble_type, result in ensemble_results.items():\n",
    "            self.save_ensemble_performance(\n",
    "                symbol, ensemble_type, result['metrics'],\n",
    "                train_samples, test_samples, model_list, has_sentiment\n",
    "            )\n",
    "        \n",
    "        # Save ensemble data to file\n",
    "        self.save_ensemble_data(symbol, ensemble_results)\n",
    "        \n",
    "        return ensemble_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Initialize Ensemble Trainer\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INITIALIZING ENSEMBLE MODEL TRAINER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ensemble_trainer = EnsembleModels(db_config)\n",
    "print(\"✓ Ensemble trainer initialized\")\n",
    "print(\"✓ Will create 4 ensemble methods:\")\n",
    "print(\"  1. Simple Average - Equal weight to all models\")\n",
    "print(\"  2. Weighted Average - Weight by directional accuracy\")\n",
    "print(\"  3. Top-3 Models - Use only best 3 models\")\n",
    "print(\"  4. Median - Robust to outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Load Enhanced Model Results\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING ENHANCED MODEL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the enhanced stock data with predictions\n",
    "try:\n",
    "    with open('enhanced_stock_data.pkl', 'rb') as f:\n",
    "        enhanced_stock_data = pickle.load(f)\n",
    "    print(f\"✓ Loaded data for {len(enhanced_stock_data)} stocks\")\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ enhanced_stock_data.pkl not found!\")\n",
    "    enhanced_stock_data = {}\n",
    "    \n",
    "def load_all_model_predictions(symbol, stock_data):\n",
    "    \"\"\"Load predictions from all trained models for ensemble\"\"\"\n",
    "    import joblib\n",
    "    \n",
    "    model_dir = 'enhanced_models'\n",
    "    predictions = {}\n",
    "    \n",
    "    # Model types to load\n",
    "    model_types = ['RandomForest', 'XGBoost', 'MLP', 'Lasso', 'Ridge', 'ARIMA']\n",
    "    \n",
    "    X_test = stock_data['X_test']\n",
    "    y_test = stock_data['y_test']\n",
    "    \n",
    "    for model_type in model_types:\n",
    "        try:\n",
    "            model_path = f\"{model_dir}/{symbol}_{model_type}_enhanced.pkl\"\n",
    "            if os.path.exists(model_path):\n",
    "                model = joblib.load(model_path)\n",
    "                \n",
    "                # Make predictions\n",
    "                if model_type == 'ARIMA':\n",
    "                    # ARIMA predictions are different\n",
    "                    try:\n",
    "                        y_pred = model.forecast(steps=len(y_test))\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    y_pred = model.predict(X_test)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "                rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                \n",
    "                # Directional accuracy\n",
    "                true_direction = y_test > 0\n",
    "                pred_direction = y_pred > 0\n",
    "                dir_acc = (np.sum(true_direction == pred_direction) / len(true_direction)) * 100\n",
    "                \n",
    "                predictions[model_type] = {\n",
    "                    'predictions': y_pred,\n",
    "                    'metrics': {\n",
    "                        'rmse': rmse,\n",
    "                        'mae': mae,\n",
    "                        'directional_accuracy': dir_acc\n",
    "                    },\n",
    "                    'y_test': y_test\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not load {model_type} for {symbol}: {str(e)[:50]}\")\n",
    "            continue\n",
    "    \n",
    "    return predictions if predictions else None\n",
    "\n",
    "print(\"✓ Helper function to load model predictions ready\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Test Ensemble on Single Stock\n",
    "# ============================================================================\n",
    "\n",
    "if enhanced_stock_data:\n",
    "    test_symbol = list(enhanced_stock_data.keys())[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TESTING ENSEMBLE METHODS ON {test_symbol}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load model predictions\n",
    "    print(f\"\\nLoading trained models for {test_symbol}...\")\n",
    "    model_predictions = load_all_model_predictions(test_symbol, enhanced_stock_data[test_symbol])\n",
    "    \n",
    "    if model_predictions:\n",
    "        print(f\"✓ Loaded {len(model_predictions)} models\")\n",
    "        print(f\"  Models: {list(model_predictions.keys())}\")\n",
    "        \n",
    "        # Create ensembles\n",
    "        print(\"\\nCreating ensemble predictions...\")\n",
    "        ensemble_results = ensemble_trainer.create_ensemble_predictions(\n",
    "            test_symbol, \n",
    "            model_predictions\n",
    "        )\n",
    "        \n",
    "        if ensemble_results:\n",
    "            print(\"\\n\" + \"-\"*70)\n",
    "            print(\"ENSEMBLE PERFORMANCE COMPARISON\")\n",
    "            print(\"-\"*70)\n",
    "            print(f\"{'Method':<20} {'RMSE (%)':<12} {'MAE (%)':<12} {'Dir Acc'}\")\n",
    "            print(\"-\"*70)\n",
    "            \n",
    "            # Show individual models first\n",
    "            print(\"\\nIndividual Models:\")\n",
    "            for model_name, data in model_predictions.items():\n",
    "                metrics = data['metrics']\n",
    "                print(f\"{model_name:<20} {metrics['rmse']:>8.3f}%    \"\n",
    "                      f\"{metrics['mae']:>8.3f}%    {metrics['directional_accuracy']:>8.1f}%\")\n",
    "            \n",
    "            # Show ensemble results\n",
    "            print(\"\\nEnsemble Methods:\")\n",
    "            for ensemble_name, data in ensemble_results.items():\n",
    "                metrics = data['metrics']\n",
    "                print(f\"{ensemble_name:<20} {metrics['rmse']:>8.3f}%    \"\n",
    "                      f\"{metrics['mae']:>8.3f}%    {metrics['directional_accuracy']:>8.1f}%\")\n",
    "            \n",
    "            # Find best method\n",
    "            all_methods = {**model_predictions, **ensemble_results}\n",
    "            best_method = max(all_methods.items(), \n",
    "                            key=lambda x: x[1]['metrics']['directional_accuracy'])\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(f\"✓ Best method: {best_method[0]}\")\n",
    "            print(f\"  Directional Accuracy: {best_method[1]['metrics']['directional_accuracy']:.2f}%\")\n",
    "            print(f\"  RMSE: {best_method[1]['metrics']['rmse']:.3f}%\")\n",
    "        else:\n",
    "            print(\"✗ Failed to create ensemble predictions\")\n",
    "    else:\n",
    "        print(f\"✗ No model predictions available for {test_symbol}\")\n",
    "else:\n",
    "    print(\"\\n✗ No stock data loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Train Ensembles for All Stocks\n",
    "# ============================================================================\n",
    "\n",
    "if enhanced_stock_data:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING ENSEMBLE MODELS FOR ALL STOCKS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Processing {len(enhanced_stock_data)} stocks...\")\n",
    "    print(\"This will take 5-10 minutes...\\n\")\n",
    "    \n",
    "    all_ensemble_results = {}\n",
    "    ensemble_summary = []\n",
    "    \n",
    "    for idx, (symbol, stock_data) in enumerate(enhanced_stock_data.items(), 1):\n",
    "        print(f\"\\n[{idx}/{len(enhanced_stock_data)}] Creating ensembles for {symbol}...\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        try:\n",
    "            # Load model predictions\n",
    "            model_predictions = load_all_model_predictions(symbol, stock_data)\n",
    "            \n",
    "            if not model_predictions or len(model_predictions) < 2:\n",
    "                print(f\"  ✗ Insufficient models ({len(model_predictions) if model_predictions else 0}) for ensemble\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  ✓ Loaded {len(model_predictions)} models\")\n",
    "            \n",
    "            # Create ensembles\n",
    "            ensemble_results = ensemble_trainer.train_all_ensembles(\n",
    "                symbol, \n",
    "                model_predictions,\n",
    "                len(stock_data['X_train']),\n",
    "                len(stock_data['X_test']),\n",
    "                stock_data.get('has_sentiment', False)\n",
    "            )\n",
    "            \n",
    "            if ensemble_results:\n",
    "                all_ensemble_results[symbol] = ensemble_results\n",
    "                \n",
    "                # Print performance\n",
    "                for ensemble_type, data in ensemble_results.items():\n",
    "                    metrics = data['metrics']\n",
    "                    print(f\"  {ensemble_type:20s} | RMSE: {metrics['rmse']:6.2f}% | \"\n",
    "                          f\"Dir Acc: {metrics['directional_accuracy']:5.1f}%\")\n",
    "                    \n",
    "                    ensemble_summary.append({\n",
    "                        'symbol': symbol,\n",
    "                        'ensemble_type': ensemble_type,\n",
    "                        'rmse': metrics['rmse'],\n",
    "                        'mae': metrics['mae'],\n",
    "                        'dir_acc': metrics['directional_accuracy'],\n",
    "                        'has_sentiment': stock_data.get('has_sentiment', False)\n",
    "                    })\n",
    "                \n",
    "                # Show best ensemble for this stock\n",
    "                best_ensemble = max(ensemble_results.items(), \n",
    "                                  key=lambda x: x[1]['metrics']['directional_accuracy'])\n",
    "                print(f\"  → Best: {best_ensemble[0]} ({best_ensemble[1]['metrics']['directional_accuracy']:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  ✗ Failed to create ensembles\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {str(e)[:70]}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ENSEMBLE TRAINING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Save all ensemble results\n",
    "    with open('ensemble_results.pkl', 'wb') as f:\n",
    "        pickle.dump(all_ensemble_results, f)\n",
    "    print(\"\\n✓ Saved ensemble results to: ensemble_results.pkl\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n✗ No stock data available for ensemble training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Ensemble Performance Analysis\n",
    "# ============================================================================\n",
    "\n",
    "if ensemble_summary:\n",
    "    ensemble_df = pd.DataFrame(ensemble_summary)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ENSEMBLE PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Overall statistics by ensemble type\n",
    "    print(\"\\n1. AVERAGE PERFORMANCE BY ENSEMBLE TYPE\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    ensemble_stats = ensemble_df.groupby('ensemble_type').agg({\n",
    "        'rmse': ['mean', 'std', 'min', 'max'],\n",
    "        'dir_acc': ['mean', 'std', 'min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    print(ensemble_stats)\n",
    "    \n",
    "    # Best ensemble per stock\n",
    "    print(\"\\n2. BEST ENSEMBLE METHOD PER STOCK\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    best_per_stock = ensemble_df.loc[ensemble_df.groupby('symbol')['dir_acc'].idxmax()]\n",
    "    print(f\"{'Symbol':<10} {'Best Method':<20} {'Dir Acc':<10} {'RMSE'}\")\n",
    "    print(\"-\"*70)\n",
    "    for _, row in best_per_stock.head(10).iterrows():\n",
    "        print(f\"{row['symbol']:<10} {row['ensemble_type']:<20} \"\n",
    "              f\"{row['dir_acc']:>6.1f}%    {row['rmse']:>6.2f}%\")\n",
    "    \n",
    "    # Ensemble method ranking\n",
    "    print(\"\\n3. ENSEMBLE METHOD RANKING (by avg directional accuracy)\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    method_ranking = ensemble_df.groupby('ensemble_type')['dir_acc'].mean().sort_values(ascending=False)\n",
    "    for method, avg_acc in method_ranking.items():\n",
    "        print(f\"{method:<25} {avg_acc:>6.2f}%\")\n",
    "    \n",
    "    # Improvement over best individual model\n",
    "    print(\"\\n4. ENSEMBLE VS INDIVIDUAL MODELS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    conn = mysql.connector.connect(**db_config)\n",
    "    \n",
    "    # Get best individual model performance per stock\n",
    "    individual_query = \"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        MAX(directional_accuracy) as best_individual_acc,\n",
    "        MIN(rmse) as best_individual_rmse\n",
    "    FROM model_performance\n",
    "    WHERE model_type NOT LIKE 'Ensemble%'\n",
    "    GROUP BY symbol\n",
    "    \"\"\"\n",
    "    \n",
    "    individual_best = pd.read_sql(individual_query, conn)\n",
    "    \n",
    "    # Get best ensemble performance per stock\n",
    "    ensemble_query = \"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        MAX(directional_accuracy) as best_ensemble_acc,\n",
    "        MIN(rmse) as best_ensemble_rmse\n",
    "    FROM model_performance\n",
    "    WHERE model_type LIKE 'Ensemble%'\n",
    "    GROUP BY symbol\n",
    "    \"\"\"\n",
    "    \n",
    "    ensemble_best = pd.read_sql(ensemble_query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Merge and compare\n",
    "    comparison = pd.merge(individual_best, ensemble_best, on='symbol', how='inner')\n",
    "    comparison['acc_improvement'] = comparison['best_ensemble_acc'] - comparison['best_individual_acc']\n",
    "    comparison['rmse_improvement'] = comparison['best_individual_rmse'] - comparison['best_ensemble_rmse']\n",
    "    \n",
    "    print(f\"\\nStocks where ensemble IMPROVED accuracy:\")\n",
    "    improved = comparison[comparison['acc_improvement'] > 0].sort_values('acc_improvement', ascending=False)\n",
    "    print(f\"{'Symbol':<10} {'Individual':<12} {'Ensemble':<12} {'Improvement'}\")\n",
    "    print(\"-\"*70)\n",
    "    for _, row in improved.head(10).iterrows():\n",
    "        print(f\"{row['symbol']:<10} {row['best_individual_acc']:>8.2f}%    \"\n",
    "              f\"{row['best_ensemble_acc']:>8.2f}%    {row['acc_improvement']:>+6.2f}%\")\n",
    "    \n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"  Stocks with improved accuracy: {len(improved)} / {len(comparison)}\")\n",
    "    print(f\"  Average accuracy improvement: {comparison['acc_improvement'].mean():+.2f}%\")\n",
    "    print(f\"  Average RMSE improvement: {comparison['rmse_improvement'].mean():+.3f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✗ No ensemble summary data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Save Summary Report\n",
    "# ============================================================================\n",
    "\n",
    "if ensemble_summary:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING SUMMARY REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create comprehensive report\n",
    "    report = {\n",
    "        'ensemble_df': ensemble_df,\n",
    "        'method_ranking': method_ranking,\n",
    "        'best_per_stock': best_per_stock,\n",
    "        'comparison': comparison if 'comparison' in locals() else None,\n",
    "        'timestamp': datetime.now()\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    with open('ensemble_report.pkl', 'wb') as f:\n",
    "        pickle.dump(report, f)\n",
    "    \n",
    "    # Save CSV for easy viewing\n",
    "    ensemble_df.to_csv('ensemble_performance.csv', index=False)\n",
    "    \n",
    "    print(\"✓ Saved ensemble report to: ensemble_report.pkl\")\n",
    "    print(\"✓ Saved performance CSV to: ensemble_performance.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
