{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8025b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded\n",
      "PyTorch version: 2.7.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 1: Import Dependencies ---\n",
    "\n",
    "import praw\n",
    "import requests\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import logging\n",
    "from textblob import TextBlob\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Logging Setup ---\n",
    "# Setup logging first to capture potential errors early\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        # Use 'utf-8' encoding for the file handler\n",
    "        logging.FileHandler('enhanced_sentiment_scraper.log', encoding='utf-8'),\n",
    "        logging.StreamHandler() # Console handler might still error depending on console settings\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- MODIFIED: safe_print function ---\n",
    "def safe_print(*args, **kwargs):\n",
    "    \"\"\"Prints messages, falling back to basic encoding if Unicode fails.\"\"\"\n",
    "    try:\n",
    "        print(*args, **kwargs)\n",
    "    except UnicodeEncodeError:\n",
    "        sep = kwargs.get('sep', ' ')\n",
    "        end = kwargs.get('end', '\\n')\n",
    "        message = sep.join(map(str, args)) + end\n",
    "        cleaned_message = message.encode('utf-8', 'replace').decode('cp1252', 'replace')\n",
    "        print(cleaned_message, end='')\n",
    "\n",
    "safe_print(\"Dependencies loaded\")\n",
    "safe_print(f\"PyTorch version: {torch.__version__}\")\n",
    "# --- END CELL 1 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27e497e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations loaded\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 2: Configurations ---\n",
    "\n",
    "# Reddit API Configuration\n",
    "reddit_config = {\n",
    "    'client_id': '6SD-D4KilOopu4O6m9R9VA',\n",
    "    'client_secret': 'mv-Z3_vFTC7WApSWvtdxUX0MKDq3QQ',\n",
    "    'user_agent': 'Trading Sentiment Bot by Delicious_Divide6891'\n",
    "}\n",
    "\n",
    "# News API Key\n",
    "news_api_key = '73e9447f080543c3885ec7803f705101'\n",
    "\n",
    "# Database Configuration\n",
    "db_config = {\n",
    "    'host': '127.0.0.1',\n",
    "    'user': 'root',\n",
    "    'password': '',\n",
    "    'database': 'trading_system'\n",
    "}\n",
    "\n",
    "safe_print(\"Configurations loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 3: Sentiment Scraper Class ---\n",
    "\n",
    "class EnhancedSentimentScraper:\n",
    "    def __init__(self, reddit_config, news_api_key, db_config):\n",
    "        self.reddit_config = reddit_config\n",
    "        self.news_api_key = news_api_key\n",
    "        self.db_config = db_config\n",
    "        # Logging is already set up globally\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.reddit = None\n",
    "        self.finbert_model = None\n",
    "        self.finbert_tokenizer = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Removed setup_logging method as it's now global\n",
    "\n",
    "    def connect_db(self):\n",
    "        \"\"\"Connect to MySQL database\"\"\"\n",
    "        try:\n",
    "            conn = mysql.connector.connect(**self.db_config)\n",
    "            return conn\n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Database connection error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_finbert(self):\n",
    "        \"\"\"Load FinBERT model for financial sentiment analysis\"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Loading FinBERT model...\")\n",
    "            model_name = \"ProsusAI/finbert\"\n",
    "            self.finbert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.finbert_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "            self.finbert_model.to(self.device)\n",
    "            self.finbert_model.eval()\n",
    "            self.logger.info(f\"FinBERT loaded successfully on {self.device}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading FinBERT: {e}\")\n",
    "            return False\n",
    "\n",
    "    # --- MODIFIED: create_sentiment_tables ---\n",
    "    def create_sentiment_tables(self):\n",
    "        \"\"\"Create or update tables for dual sentiment analysis, compatible with older MySQL.\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        db_name = self.db_config['database'] # Get database name for information_schema query\n",
    "\n",
    "        # 1. Create reddit_sentiment table (if not exists)\n",
    "        reddit_table_create = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS reddit_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY, symbol VARCHAR(10), subreddit VARCHAR(50), post_id VARCHAR(20) UNIQUE,\n",
    "            title TEXT, selftext TEXT, score INT, num_comments INT, created_utc TIMESTAMP,\n",
    "            textblob_score DECIMAL(5,4), textblob_label VARCHAR(20), finbert_score DECIMAL(5,4), finbert_label VARCHAR(20),\n",
    "            finbert_confidence DECIMAL(5,4), combined_score DECIMAL(5,4), combined_label VARCHAR(20),\n",
    "            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, INDEX idx_symbol (symbol), INDEX idx_created (created_utc),\n",
    "            FOREIGN KEY (symbol) REFERENCES stocks(symbol) ON DELETE CASCADE ON UPDATE CASCADE\n",
    "        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n",
    "        \"\"\"\n",
    "\n",
    "        # 2. Create news_sentiment table (if not exists)\n",
    "        news_table_create = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS news_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY, symbol VARCHAR(10), source VARCHAR(100), title TEXT, description TEXT,\n",
    "            published_at TIMESTAMP NULL, textblob_score DECIMAL(5,4), textblob_label VARCHAR(20), finbert_score DECIMAL(5,4),\n",
    "            finbert_label VARCHAR(20), finbert_confidence DECIMAL(5,4), combined_score DECIMAL(5,4), combined_label VARCHAR(20),\n",
    "            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, INDEX idx_symbol (symbol), INDEX idx_published (published_at),\n",
    "             FOREIGN KEY (symbol) REFERENCES stocks(symbol) ON DELETE CASCADE ON UPDATE CASCADE\n",
    "       ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n",
    "        \"\"\"\n",
    "\n",
    "        # 3. Create daily_sentiment table (if not exists) - Basic structure first\n",
    "        daily_sentiment_create = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS daily_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            symbol VARCHAR(10),\n",
    "            date DATE,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE KEY unique_symbol_date (symbol, date),\n",
    "            FOREIGN KEY (symbol) REFERENCES stocks(symbol) ON DELETE CASCADE ON UPDATE CASCADE\n",
    "        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n",
    "        \"\"\"\n",
    "\n",
    "        # Columns to ensure exist in daily_sentiment\n",
    "        required_columns = {\n",
    "            \"reddit_textblob_avg\": \"DECIMAL(5,4) NULL DEFAULT NULL\",\n",
    "            \"reddit_finbert_avg\": \"DECIMAL(5,4) NULL DEFAULT NULL\",\n",
    "            \"reddit_combined_avg\": \"DECIMAL(5,4) NULL DEFAULT NULL\",\n",
    "            \"reddit_post_count\": \"INT DEFAULT 0\",\n",
    "            \"news_textblob_avg\": \"DECIMAL(5,4) NULL DEFAULT NULL\",\n",
    "            \"news_finbert_avg\": \"DECIMAL(5,4) NULL DEFAULT NULL\",\n",
    "            \"news_combined_avg\": \"DECIMAL(5,4) NULL DEFAULT NULL\",\n",
    "            \"news_article_count\": \"INT DEFAULT 0\",\n",
    "            \"overall_textblob\": \"DECIMAL(5,4) NULL DEFAULT NULL\",\n",
    "            \"overall_finbert\": \"DECIMAL(5,4) NULL DEFAULT NULL\",\n",
    "            \"overall_combined\": \"DECIMAL(5,4) NULL DEFAULT NULL\",\n",
    "            \"total_mentions\": \"INT DEFAULT 0\"\n",
    "        }\n",
    "\n",
    "        # Columns to drop if they exist (old schema)\n",
    "        columns_to_drop = [\n",
    "            \"reddit_avg_sentiment\",\n",
    "            \"news_avg_sentiment\",\n",
    "            \"combined_sentiment\",\n",
    "            \"avg_sentiment_score\",\n",
    "            \"source_count\"\n",
    "        ]\n",
    "\n",
    "        tables_to_create = [\n",
    "            (\"reddit_sentiment\", reddit_table_create),\n",
    "            (\"news_sentiment\", news_table_create),\n",
    "            (\"daily_sentiment\", daily_sentiment_create)\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            # Create tables if they don't exist\n",
    "            for name, query in tables_to_create:\n",
    "                self.logger.info(f\"Executing CREATE IF NOT EXISTS for {name}...\")\n",
    "                cursor.execute(query)\n",
    "            self.logger.info(\"Initial table creation check complete.\")\n",
    "            conn.commit() # Commit creation before altering\n",
    "\n",
    "            # --- Check and Add Columns for daily_sentiment ---\n",
    "            self.logger.info(\"Checking/Updating daily_sentiment schema...\")\n",
    "            # Get existing columns\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT COLUMN_NAME\n",
    "                FROM information_schema.columns\n",
    "                WHERE table_schema = '{db_name}' AND table_name = 'daily_sentiment';\n",
    "            \"\"\")\n",
    "            existing_columns = {row[0] for row in cursor.fetchall()}\n",
    "            self.logger.info(f\"Existing columns in daily_sentiment: {existing_columns}\")\n",
    "\n",
    "            # Add missing required columns\n",
    "            for col_name, col_definition in required_columns.items():\n",
    "                if col_name not in existing_columns:\n",
    "                    try:\n",
    "                        alter_query = f\"ALTER TABLE daily_sentiment ADD COLUMN {col_name} {col_definition}\"\n",
    "                        self.logger.info(f\"Adding missing column: {col_name}\")\n",
    "                        cursor.execute(alter_query)\n",
    "                        conn.commit() # Commit after each successful alter add\n",
    "                    except mysql.connector.Error as e:\n",
    "                        self.logger.error(f\"Error adding column {col_name}: {e}\")\n",
    "                        conn.rollback() # Rollback if alter fails\n",
    "                #else:\n",
    "                #    self.logger.info(f\"Column '{col_name}' already exists.\")\n",
    "\n",
    "            # Drop old columns\n",
    "            for col_name in columns_to_drop:\n",
    "                if col_name in existing_columns:\n",
    "                    try:\n",
    "                        drop_query = f\"ALTER TABLE daily_sentiment DROP COLUMN {col_name}\"\n",
    "                        self.logger.info(f\"Dropping old column: {col_name}\")\n",
    "                        cursor.execute(drop_query)\n",
    "                        conn.commit() # Commit after each successful alter drop\n",
    "                    except mysql.connector.Error as e:\n",
    "                        self.logger.error(f\"Error dropping column {col_name}: {e}\")\n",
    "                        conn.rollback() # Rollback if alter fails\n",
    "\n",
    "            self.logger.info(\"daily_sentiment schema check/update complete.\")\n",
    "            self.logger.info(\"Enhanced sentiment tables created/updated successfully\")\n",
    "            return True\n",
    "\n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Error during table setup: {e}\")\n",
    "            conn.rollback()\n",
    "            return False\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "    # --- END MODIFIED: create_sentiment_tables ---\n",
    "\n",
    "    def init_reddit(self):\n",
    "        \"\"\"Initialize Reddit API connection\"\"\"\n",
    "        try:\n",
    "            self.reddit = praw.Reddit(\n",
    "                client_id=self.reddit_config['client_id'],\n",
    "                client_secret=self.reddit_config['client_secret'],\n",
    "                user_agent=self.reddit_config['user_agent']\n",
    "            )\n",
    "            self.logger.info(\"Reddit API initialized successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing Reddit API: {e}\")\n",
    "            return False\n",
    "\n",
    "    def analyze_textblob(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment using TextBlob (general-purpose)\n",
    "        Returns: (score, label)\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return 0.0, 'neutral'\n",
    "\n",
    "        try:\n",
    "            blob = TextBlob(text)\n",
    "            polarity = blob.sentiment.polarity\n",
    "\n",
    "            if polarity > 0.1:\n",
    "                label = 'positive'\n",
    "            elif polarity < -0.1:\n",
    "                label = 'negative'\n",
    "            else:\n",
    "                label = 'neutral'\n",
    "\n",
    "            return round(polarity, 4), label\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in TextBlob analysis: {e}\")\n",
    "            return 0.0, 'neutral'\n",
    "\n",
    "    def analyze_finbert(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment using FinBERT (finance-specific)\n",
    "        Returns: (score, label, confidence)\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return 0.0, 'neutral', 0.0\n",
    "\n",
    "        if self.finbert_model is None:\n",
    "            self.logger.warning(\"FinBERT not loaded, using neutral sentiment\")\n",
    "            return 0.0, 'neutral', 0.0\n",
    "\n",
    "        try:\n",
    "            # Truncate text to FinBERT's max length (512 tokens)\n",
    "            text_slice = text[:1024] # Slice potentially longer text first\n",
    "\n",
    "            # Tokenize\n",
    "            inputs = self.finbert_tokenizer(\n",
    "                text_slice,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=512, # Hard limit for the model\n",
    "                padding=True\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = self.finbert_model(**inputs)\n",
    "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "            # FinBERT outputs: [positive, negative, neutral]\n",
    "            probs = predictions[0].cpu().numpy()\n",
    "\n",
    "            # Get label and confidence\n",
    "            labels = ['positive', 'negative', 'neutral']\n",
    "            max_idx = np.argmax(probs)\n",
    "            label = labels[max_idx]\n",
    "            confidence = float(probs[max_idx])\n",
    "\n",
    "            # Calculate sentiment score: positive - negative\n",
    "            score = float(probs[0] - probs[1])\n",
    "\n",
    "            return round(score, 4), label, round(confidence, 4)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in FinBERT analysis processing text snippet '{text[:50]}...': {e}\")\n",
    "            return 0.0, 'neutral', 0.0\n",
    "\n",
    "\n",
    "    def combine_sentiments(self, textblob_score, finbert_score, finbert_conf):\n",
    "        \"\"\"\n",
    "        Combine TextBlob and FinBERT scores\n",
    "        Weight FinBERT more for financial text (70% FinBERT, 30% TextBlob)\n",
    "        \"\"\"\n",
    "        finbert_weight = 0.7\n",
    "        textblob_weight = 0.3\n",
    "\n",
    "        combined = (finbert_score * finbert_weight) + (textblob_score * textblob_weight)\n",
    "\n",
    "        if combined > 0.1:\n",
    "            label = 'positive'\n",
    "        elif combined < -0.1:\n",
    "            label = 'negative'\n",
    "        else:\n",
    "            label = 'neutral'\n",
    "\n",
    "        return round(combined, 4), label\n",
    "\n",
    "    # Uses the expanded default list of subreddits\n",
    "    def scrape_reddit(self, symbols,\n",
    "                     subreddits=['stocks', 'investing', 'wallstreetbets', 'SecurityAnalysis',\n",
    "                                 'StockMarket', 'finance', 'options', 'daytrading'],\n",
    "                     limit=100, time_filter='week'):\n",
    "        \"\"\"Scrape Reddit with dual sentiment analysis\"\"\"\n",
    "        if not self.reddit:\n",
    "            if not self.init_reddit():\n",
    "                return False\n",
    "\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        total_posts_saved = 0\n",
    "\n",
    "        self.logger.info(f\"Scraping {len(subreddits)} subreddits: {subreddits}\")\n",
    "\n",
    "        for idx, symbol in enumerate(symbols, 1):\n",
    "            safe_print(f\"\\n[{idx}/{len(symbols)}] Processing {symbol}...\")\n",
    "            symbol_posts = 0\n",
    "            for subreddit_name in subreddits:\n",
    "                try:\n",
    "                    safe_print(f\"  -> Searching r/{subreddit_name}... \", end='')\n",
    "                    subreddit = self.reddit.subreddit(subreddit_name)\n",
    "                    search_query = f\"'{symbol}' OR ${symbol} OR {symbol}\" # Broader query\n",
    "\n",
    "                    post_count = 0\n",
    "                    processed_posts = 0\n",
    "                    for post in subreddit.search(search_query, time_filter=time_filter, limit=limit, sort='new'):\n",
    "                        processed_posts += 1\n",
    "                        # Combine title and selftext\n",
    "                        full_text = f\"{post.title} {post.selftext}\"\n",
    "\n",
    "                        # Dual sentiment analysis\n",
    "                        tb_score, tb_label = self.analyze_textblob(full_text)\n",
    "                        fb_score, fb_label, fb_conf = self.analyze_finbert(full_text)\n",
    "                        combined_score, combined_label = self.combine_sentiments(tb_score, fb_score, fb_conf)\n",
    "\n",
    "                        try:\n",
    "                            query = \"\"\"\n",
    "                            INSERT IGNORE INTO reddit_sentiment\n",
    "                            (symbol, subreddit, post_id, title, selftext, score, num_comments,\n",
    "                             created_utc, textblob_score, textblob_label, finbert_score,\n",
    "                             finbert_label, finbert_confidence, combined_score, combined_label)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                            \"\"\"\n",
    "                            values = (\n",
    "                                symbol, subreddit_name, post.id,\n",
    "                                post.title[:500], post.selftext[:1000] if post.selftext else '',\n",
    "                                post.score, post.num_comments, datetime.fromtimestamp(post.created_utc),\n",
    "                                tb_score, tb_label, fb_score, fb_label, fb_conf,\n",
    "                                combined_score, combined_label\n",
    "                            )\n",
    "                            cursor.execute(query, values)\n",
    "                            if cursor.rowcount > 0: # Check if insert actually happened\n",
    "                                total_posts_saved += 1\n",
    "                                symbol_posts += 1\n",
    "                            post_count += 1 # Count posts processed, not just saved\n",
    "\n",
    "                        except mysql.connector.Error as e:\n",
    "                            # Log only if it's not a duplicate entry error\n",
    "                            if \"Duplicate entry\" not in str(e):\n",
    "                                self.logger.error(f\"DB Error inserting Reddit post {post.id}: {e}\")\n",
    "                        except Exception as inner_e:\n",
    "                             self.logger.error(f\"Unexpected error processing post {post.id}: {inner_e}\")\n",
    "\n",
    "\n",
    "                    safe_print(f\"{processed_posts} posts processed.\")\n",
    "                    time.sleep(2)  # Adhere to Reddit API rate limits\n",
    "\n",
    "                except praw.exceptions.PRAWException as e:\n",
    "                    self.logger.error(f\"PRAW Error scraping r/{subreddit_name} for {symbol}: {e}\")\n",
    "                    safe_print(f\" Error scraping r/{subreddit_name}.\")\n",
    "                    time.sleep(5) # Longer sleep on API error\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"General Error scraping r/{subreddit_name} for {symbol}: {e}\")\n",
    "                    safe_print(f\" Error scraping r/{subreddit_name}.\")\n",
    "                    time.sleep(5) # Longer sleep on error\n",
    "                    continue # Try next subreddit\n",
    "            safe_print(f\"  Saved {symbol_posts} new posts for {symbol}.\")\n",
    "\n",
    "        try:\n",
    "            conn.commit()\n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Error committing Reddit data: {e}\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "        self.logger.info(f\"Finished Reddit scrape. Saved {total_posts_saved} new posts overall.\")\n",
    "        return True\n",
    "\n",
    "\n",
    "    def scrape_news(self, symbols, days_back=7):\n",
    "        \"\"\"Scrape news with dual sentiment analysis\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        if not conn: return False\n",
    "        cursor = conn.cursor()\n",
    "        total_articles_saved = 0\n",
    "        to_date = datetime.now()\n",
    "        from_date = to_date - timedelta(days=days_back)\n",
    "\n",
    "        for idx, symbol in enumerate(symbols, 1):\n",
    "            safe_print(f\"\\n[{idx}/{len(symbols)}] Fetching news for {symbol}...\")\n",
    "            symbol_articles = 0\n",
    "            retries = 3\n",
    "            while retries > 0:\n",
    "                try:\n",
    "                    url = 'https://newsapi.org/v2/everything'\n",
    "                    params = {\n",
    "                        'q': f'\"{symbol} stock\" OR \"{symbol} shares\"',\n",
    "                        'from': from_date.strftime('%Y-%m-%d'),\n",
    "                        'to': to_date.strftime('%Y-%m-%d'),\n",
    "                        'language': 'en', 'sortBy': 'publishedAt',\n",
    "                        'apiKey': self.news_api_key, 'pageSize': 100\n",
    "                    }\n",
    "                    response = requests.get(url, params=params, timeout=20) # Added timeout\n",
    "                    response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "                    data = response.json()\n",
    "                    articles = data.get('articles', [])\n",
    "                    processed_articles = 0\n",
    "\n",
    "                    for article in articles:\n",
    "                        processed_articles += 1\n",
    "                        text = f\"{article.get('title', '')} {article.get('description', '')}\"\n",
    "                        tb_score, tb_label = self.analyze_textblob(text)\n",
    "                        fb_score, fb_label, fb_conf = self.analyze_finbert(text)\n",
    "                        combined_score, combined_label = self.combine_sentiments(tb_score, fb_score, fb_conf)\n",
    "\n",
    "                        try:\n",
    "                            query = \"\"\"\n",
    "                            INSERT IGNORE INTO news_sentiment\n",
    "                            (symbol, source, title, description, published_at, textblob_score, textblob_label,\n",
    "                             finbert_score, finbert_label, finbert_confidence, combined_score, combined_label)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                            \"\"\"\n",
    "                            published = article.get('publishedAt', '')\n",
    "                            published_dt = datetime.now() # Default value\n",
    "                            if published:\n",
    "                                try:\n",
    "                                    # Handle different possible formats, prioritize ISO with Z\n",
    "                                    if 'Z' in published:\n",
    "                                       published_dt = datetime.strptime(published, '%Y-%m-%dT%H:%M:%SZ')\n",
    "                                    elif '+' in published: # Handle timezone offset like +00:00\n",
    "                                        published_dt = datetime.strptime(published, '%Y-%m-%dT%H:%M:%S%z')\n",
    "                                    else: # Assume UTC if no timezone info\n",
    "                                        published_dt = datetime.strptime(published, '%Y-%m-%dT%H:%M:%S')\n",
    "                                except ValueError:\n",
    "                                     self.logger.warning(f\"Could not parse date '{published}' for {symbol}, using current time.\")\n",
    "\n",
    "                            values = (\n",
    "                                symbol, article.get('source', {}).get('name', 'Unknown')[:100],\n",
    "                                article.get('title', '')[:500], article.get('description', '')[:1000],\n",
    "                                published_dt.replace(tzinfo=None), # Store timezone-naive in DB\n",
    "                                tb_score, tb_label, fb_score, fb_label, fb_conf,\n",
    "                                combined_score, combined_label\n",
    "                            )\n",
    "                            cursor.execute(query, values)\n",
    "                            if cursor.rowcount > 0:\n",
    "                                total_articles_saved += 1\n",
    "                                symbol_articles += 1\n",
    "\n",
    "                        except mysql.connector.Error as e:\n",
    "                            if \"Duplicate entry\" not in str(e):\n",
    "                                self.logger.error(f\"DB Error inserting news article for {symbol}: {e}\")\n",
    "                        except Exception as inner_e:\n",
    "                            self.logger.error(f\"Unexpected error processing article for {symbol}: {inner_e}\")\n",
    "\n",
    "                    safe_print(f\"  Processed {processed_articles} articles. Saved {symbol_articles} new.\")\n",
    "                    break # Success, exit retry loop\n",
    "\n",
    "                except requests.exceptions.HTTPError as e:\n",
    "                    if e.response.status_code == 426: # Free plan limit\n",
    "                         self.logger.warning(\"NewsAPI free plan limit likely reached. Upgrade for more results.\")\n",
    "                         safe_print(\" NewsAPI limit reached.\")\n",
    "                         retries = 0 # Don't retry rate limit errors\n",
    "                         break # Stop processing this symbol\n",
    "                    elif e.response.status_code == 429: # Rate limit\n",
    "                         self.logger.warning(f\"NewsAPI rate limit hit for {symbol}. Retrying in 5s... ({retries-1} left)\")\n",
    "                         safe_print(\" Rate limit hit, retrying...\")\n",
    "                         time.sleep(5)\n",
    "                         retries -= 1\n",
    "                    else:\n",
    "                         self.logger.error(f\"NewsAPI HTTP error for {symbol}: {e.response.status_code} - {e.response.text[:100]}\")\n",
    "                         safe_print(f\" HTTP Error {e.response.status_code}.\")\n",
    "                         retries = 0 # Don't retry other HTTP errors immediately\n",
    "                         break\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    self.logger.error(f\"NewsAPI Request error for {symbol}: {e}\")\n",
    "                    safe_print(\" Network error, retrying...\")\n",
    "                    time.sleep(5)\n",
    "                    retries -= 1\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"General Error scraping news for {symbol}: {e}\")\n",
    "                    safe_print(\" General error.\")\n",
    "                    retries = 0 \n",
    "                    break \n",
    "            # End of while loop\n",
    "            if retries == 0 and symbol_articles == 0 :\n",
    "                 safe_print(f\"  Failed to fetch news for {symbol} after retries.\")\n",
    "            time.sleep(1.5) # Pause between symbols\n",
    "\n",
    "        try:\n",
    "            conn.commit()\n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Error committing News data: {e}\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "        self.logger.info(f\"Finished News scrape. Saved {total_articles_saved} new articles overall.\")\n",
    "        return True\n",
    "\n",
    "\n",
    "    def aggregate_daily_sentiment(self, date_to_aggregate=None):\n",
    "        \"\"\"Aggregate dual sentiment data by symbol and date\"\"\"\n",
    "        if date_to_aggregate is None:\n",
    "            date_to_aggregate = datetime.now().date()\n",
    "\n",
    "        self.logger.info(f\"Aggregating daily sentiment for date: {date_to_aggregate}\")\n",
    "\n",
    "        conn = self.connect_db()\n",
    "        if not conn: return False\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Get list of active symbols\n",
    "        try:\n",
    "            cursor.execute(\"SELECT DISTINCT symbol FROM stocks WHERE is_active = 1\")\n",
    "            symbols = [row[0] for row in cursor.fetchall()]\n",
    "            if not symbols:\n",
    "                self.logger.warning(\"No active symbols found for aggregation.\")\n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "                return False\n",
    "        except mysql.connector.Error as e:\n",
    "             self.logger.error(f\"Error fetching active symbols for aggregation: {e}\")\n",
    "             cursor.close()\n",
    "             conn.close()\n",
    "             return False\n",
    "\n",
    "\n",
    "        aggregated_count = 0\n",
    "        for symbol in symbols:\n",
    "            try:\n",
    "                # Aggregate Reddit sentiment\n",
    "                reddit_query = \"\"\"\n",
    "                SELECT AVG(textblob_score), AVG(finbert_score), AVG(combined_score), COUNT(*)\n",
    "                FROM reddit_sentiment WHERE symbol = %s AND DATE(created_utc) = %s\n",
    "                \"\"\"\n",
    "                cursor.execute(reddit_query, (symbol, date_to_aggregate))\n",
    "                r_res = cursor.fetchone()\n",
    "                r_tb, r_fb, r_comb, r_count = (r or 0 for r in (r_res if r_res else (0,0,0,0)))\n",
    "\n",
    "\n",
    "                # Aggregate News sentiment\n",
    "                news_query = \"\"\"\n",
    "                SELECT AVG(textblob_score), AVG(finbert_score), AVG(combined_score), COUNT(*)\n",
    "                FROM news_sentiment WHERE symbol = %s AND DATE(published_at) = %s\n",
    "                \"\"\"\n",
    "                cursor.execute(news_query, (symbol, date_to_aggregate))\n",
    "                n_res = cursor.fetchone()\n",
    "                n_tb, n_fb, n_comb, n_count = (n or 0 for n in (n_res if n_res else (0,0,0,0)))\n",
    "\n",
    "                # Calculate overall sentiment (weighted by count)\n",
    "                total_count = r_count + n_count\n",
    "                o_tb = o_fb = o_comb = 0\n",
    "                if total_count > 0:\n",
    "                    o_tb = ((r_tb * r_count) + (n_tb * n_count)) / total_count\n",
    "                    o_fb = ((r_fb * r_count) + (n_fb * n_count)) / total_count\n",
    "                    o_comb = ((r_comb * r_count) + (n_comb * n_count)) / total_count\n",
    "\n",
    "                # Insert or update aggregated data\n",
    "                insert_query = \"\"\"\n",
    "                INSERT INTO daily_sentiment\n",
    "                (symbol, date, reddit_textblob_avg, reddit_finbert_avg, reddit_combined_avg, reddit_post_count,\n",
    "                 news_textblob_avg, news_finbert_avg, news_combined_avg, news_article_count,\n",
    "                 overall_textblob, overall_finbert, overall_combined, total_mentions)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON DUPLICATE KEY UPDATE\n",
    "                    reddit_textblob_avg=VALUES(reddit_textblob_avg), reddit_finbert_avg=VALUES(reddit_finbert_avg),\n",
    "                    reddit_combined_avg=VALUES(reddit_combined_avg), reddit_post_count=VALUES(reddit_post_count),\n",
    "                    news_textblob_avg=VALUES(news_textblob_avg), news_finbert_avg=VALUES(news_finbert_avg),\n",
    "                    news_combined_avg=VALUES(news_combined_avg), news_article_count=VALUES(news_article_count),\n",
    "                    overall_textblob=VALUES(overall_textblob), overall_finbert=VALUES(overall_finbert),\n",
    "                    overall_combined=VALUES(overall_combined), total_mentions=VALUES(total_mentions)\n",
    "                \"\"\"\n",
    "                cursor.execute(insert_query, (\n",
    "                    symbol, date_to_aggregate, r_tb, r_fb, r_comb, r_count,\n",
    "                    n_tb, n_fb, n_comb, n_count, o_tb, o_fb, o_comb, total_count\n",
    "                ))\n",
    "                aggregated_count += 1\n",
    "\n",
    "            except mysql.connector.Error as e:\n",
    "                self.logger.error(f\"Error aggregating sentiment for {symbol} on {date_to_aggregate}: {e}\")\n",
    "                conn.rollback() \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Unexpected error aggregating {symbol} on {date_to_aggregate}: {e}\")\n",
    "                conn.rollback()\n",
    "\n",
    "        try:\n",
    "            conn.commit() \n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Final commit error during aggregation: {e}\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "        self.logger.info(f\"Finished aggregation for {date_to_aggregate}. Processed {aggregated_count}/{len(symbols)} symbols.\")\n",
    "        return True\n",
    "\n",
    "\n",
    "    def get_stock_symbols(self):\n",
    "        \"\"\"Get all *active* tracked stock symbols from database\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        if not conn: return []\n",
    "        cursor = conn.cursor()\n",
    "        symbols = []\n",
    "        try:\n",
    "            cursor.execute(\"SELECT symbol FROM stocks WHERE is_active = 1 ORDER BY symbol\")\n",
    "            symbols = [row[0] for row in cursor.fetchall()]\n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Error fetching active symbols: {e}\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "        if not symbols:\n",
    "            self.logger.warning(\"No active stocks found in database. Run '01_data_collection_FIXED.py' first.\")\n",
    "        return symbols\n",
    "\n",
    "# --- END CELL 3 ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69735264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 01:05:23,276 - INFO - Loading FinBERT model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INITIALIZING ENHANCED SENTIMENT SCRAPER\n",
      "======================================================================\n",
      "\n",
      "Loading FinBERT model (this may take 1-2 minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 01:05:25,055 - INFO - FinBERT loaded successfully on cpu\n",
      "2025-10-26 01:05:25,105 - INFO - Executing CREATE IF NOT EXISTS for reddit_sentiment...\n",
      "2025-10-26 01:05:25,121 - INFO - Executing CREATE IF NOT EXISTS for news_sentiment...\n",
      "2025-10-26 01:05:25,128 - INFO - Executing CREATE IF NOT EXISTS for daily_sentiment...\n",
      "2025-10-26 01:05:25,135 - INFO - Initial table creation check complete.\n",
      "2025-10-26 01:05:25,139 - INFO - Checking/Updating daily_sentiment schema...\n",
      "2025-10-26 01:05:25,155 - INFO - Existing columns in daily_sentiment: {'overall_combined', 'reddit_textblob_avg', 'news_combined_avg', 'news_article_count', 'reddit_post_count', 'total_mentions', 'date', 'created_at', 'news_finbert_avg', 'reddit_finbert_avg', 'symbol', 'id', 'news_textblob_avg', 'overall_finbert', 'overall_textblob', 'reddit_combined_avg'}\n",
      "2025-10-26 01:05:25,155 - INFO - daily_sentiment schema check/update complete.\n",
      "2025-10-26 01:05:25,155 - INFO - Enhanced sentiment tables created/updated successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT loaded successfully\n",
      "\n",
      "Creating/Updating sentiment tables in database...\n",
      "Database tables are up-to-date.\n",
      "\n",
      "Fetching active stocks...\n",
      "\n",
      "Setup complete! Tracking 20 stocks: ['AAPL', 'AMZN', 'BLK', 'ERO', 'FXP', 'GOOGL', 'GXC', 'JPM', 'KR', 'MDT', 'META', 'MSFT', 'NVDA', 'OXY', 'PGJ', 'RSP', 'SPY', 'TSLA', 'VGK', 'XPP']\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 4: Initialize Enhanced Scraper ---\n",
    "\n",
    "# ============================================================================\n",
    "safe_print(\"\\n\" + \"=\"*70)\n",
    "safe_print(\"INITIALIZING ENHANCED SENTIMENT SCRAPER\")\n",
    "safe_print(\"=\"*70)\n",
    "\n",
    "scraper = EnhancedSentimentScraper(reddit_config, news_api_key, db_config)\n",
    "\n",
    "# Load FinBERT model\n",
    "safe_print(\"\\nLoading FinBERT model (this may take 1-2 minutes)...\")\n",
    "if scraper.load_finbert():\n",
    "    safe_print(\"FinBERT loaded successfully\") # Removed checkmark\n",
    "else:\n",
    "    safe_print(\"FinBERT loading failed - will use TextBlob only\")\n",
    "\n",
    "# Create/Update sentiment tables\n",
    "safe_print(\"\\nCreating/Updating sentiment tables in database...\")\n",
    "if scraper.create_sentiment_tables():\n",
    "    safe_print(\"Database tables are up-to-date.\") # Removed checkmark\n",
    "else:\n",
    "    safe_print(\"Failed to create/update database tables!\")\n",
    "\n",
    "# Get stock symbols from database\n",
    "safe_print(\"\\nFetching active stocks...\")\n",
    "symbols = scraper.get_stock_symbols()\n",
    "safe_print(f\"\\nSetup complete! Tracking {len(symbols)} stocks: {symbols}\") # Removed checkmark\n",
    "\n",
    "# --- END CELL 4 ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abc2becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 01:05:32,742 - INFO - Reddit API initialized successfully\n",
      "2025-10-26 01:05:32,795 - INFO - Scraping 8 subreddits: ['stocks', 'investing', 'wallstreetbets', 'SecurityAnalysis', 'StockMarket', 'finance', 'options', 'daytrading']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCRAPING REDDIT WITH DUAL SENTIMENT ANALYSIS\n",
      "======================================================================\n",
      "Analyzing with TextBlob (general) + FinBERT (finance-specific)\n",
      "This will take 5-10 minutes...\n",
      "\n",
      "\n",
      "[1/20] Processing AAPL...\n",
      "  -> Searching r/stocks... 1 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 3 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 1 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 8 posts processed.\n",
      "  Saved 13 new posts for AAPL.\n",
      "\n",
      "[2/20] Processing AMZN...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 5 posts processed.\n",
      "  -> Searching r/wallstreetbets... 9 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 1 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 1 posts processed.\n",
      "  -> Searching r/daytrading... 6 posts processed.\n",
      "  Saved 16 new posts for AMZN.\n",
      "\n",
      "[3/20] Processing BLK...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 1 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 0 posts processed.\n",
      "  Saved 0 new posts for BLK.\n",
      "\n",
      "[4/20] Processing ERO...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 0 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 0 posts processed.\n",
      "  Saved 0 new posts for ERO.\n",
      "\n",
      "[5/20] Processing FXP...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 0 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 0 posts processed.\n",
      "  Saved 0 new posts for FXP.\n",
      "\n",
      "[6/20] Processing GOOGL...\n",
      "  -> Searching r/stocks... 18 posts processed.\n",
      "  -> Searching r/investing... 3 posts processed.\n",
      "  -> Searching r/wallstreetbets... 10 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 1 posts processed.\n",
      "  -> Searching r/StockMarket... 5 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 2 posts processed.\n",
      "  -> Searching r/daytrading... 11 posts processed.\n",
      "  Saved 41 new posts for GOOGL.\n",
      "\n",
      "[7/20] Processing GXC...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 0 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 0 posts processed.\n",
      "  Saved 0 new posts for GXC.\n",
      "\n",
      "[8/20] Processing JPM...\n",
      "  -> Searching r/stocks... 1 posts processed.\n",
      "  -> Searching r/investing... 1 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 1 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 2 posts processed.\n",
      "  Saved 2 new posts for JPM.\n",
      "\n",
      "[9/20] Processing KR...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 0 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 0 posts processed.\n",
      "  Saved 0 new posts for KR.\n",
      "\n",
      "[10/20] Processing MDT...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 1 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 0 posts processed.\n",
      "  Saved 0 new posts for MDT.\n",
      "\n",
      "[11/20] Processing META...\n",
      "  -> Searching r/stocks... 3 posts processed.\n",
      "  -> Searching r/investing... 2 posts processed.\n",
      "  -> Searching r/wallstreetbets... 5 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 1 posts processed.\n",
      "  -> Searching r/StockMarket... 4 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 1 posts processed.\n",
      "  -> Searching r/daytrading... 6 posts processed.\n",
      "  Saved 13 new posts for META.\n",
      "\n",
      "[12/20] Processing MSFT...\n",
      "  -> Searching r/stocks... 2 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 3 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 1 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 1 posts processed.\n",
      "  -> Searching r/daytrading... 1 posts processed.\n",
      "  Saved 1 new posts for MSFT.\n",
      "\n",
      "[13/20] Processing NVDA...\n",
      "  -> Searching r/stocks... 4 posts processed.\n",
      "  -> Searching r/investing... 4 posts processed.\n",
      "  -> Searching r/wallstreetbets... 7 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 1 posts processed.\n",
      "  -> Searching r/StockMarket... 1 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 1 posts processed.\n",
      "  -> Searching r/daytrading... 5 posts processed.\n",
      "  Saved 13 new posts for NVDA.\n",
      "\n",
      "[14/20] Processing OXY...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 0 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 0 posts processed.\n",
      "  Saved 0 new posts for OXY.\n",
      "\n",
      "[15/20] Processing PGJ...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 0 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 0 posts processed.\n",
      "  Saved 0 new posts for PGJ.\n",
      "\n",
      "[16/20] Processing RSP...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 1 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 0 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 0 posts processed.\n",
      "  Saved 1 new posts for RSP.\n",
      "\n",
      "[17/20] Processing SPY...\n",
      "  -> Searching r/stocks... 8 posts processed.\n",
      "  -> Searching r/investing... 2 posts processed.\n",
      "  -> Searching r/wallstreetbets... 10 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 2 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 11 posts processed.\n",
      "  -> Searching r/daytrading... 16 posts processed.\n",
      "  Saved 39 new posts for SPY.\n",
      "\n",
      "[18/20] Processing TSLA...\n",
      "  -> Searching r/stocks... 1 posts processed.\n",
      "  -> Searching r/investing... 3 posts processed.\n",
      "  -> Searching r/wallstreetbets... 7 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 1 posts processed.\n",
      "  -> Searching r/StockMarket... 1 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 2 posts processed.\n",
      "  -> Searching r/daytrading... 5 posts processed.\n",
      "  Saved 9 new posts for TSLA.\n",
      "\n",
      "[19/20] Processing VGK...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 0 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 0 posts processed.\n",
      "  Saved 0 new posts for VGK.\n",
      "\n",
      "[20/20] Processing XPP...\n",
      "  -> Searching r/stocks... 0 posts processed.\n",
      "  -> Searching r/investing... 0 posts processed.\n",
      "  -> Searching r/wallstreetbets... 0 posts processed.\n",
      "  -> Searching r/SecurityAnalysis... 0 posts processed.\n",
      "  -> Searching r/StockMarket... 0 posts processed.\n",
      "  -> Searching r/finance... 0 posts processed.\n",
      "  -> Searching r/options... 0 posts processed.\n",
      "  -> Searching r/daytrading... 0 posts processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 01:13:46,053 - INFO - Finished Reddit scrape. Saved 148 new posts overall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 0 new posts for XPP.\n",
      "\n",
      "Reddit scraping complete!\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 5: Scrape Reddit with Dual Sentiment ---\n",
    "\n",
    "# ============================================================================\n",
    "safe_print(\"\\n\" + \"=\"*70)\n",
    "safe_print(\"SCRAPING REDDIT WITH DUAL SENTIMENT ANALYSIS\")\n",
    "safe_print(\"=\"*70)\n",
    "safe_print(\"Analyzing with TextBlob (general) + FinBERT (finance-specific)\")\n",
    "safe_print(\"This will take 5-10 minutes...\\n\")\n",
    "\n",
    "if symbols:\n",
    "    scraper.scrape_reddit(\n",
    "        symbols,\n",
    "        limit=50,\n",
    "        time_filter='week'\n",
    "    )\n",
    "    safe_print(\"\\nReddit scraping complete!\") # Removed checkmark\n",
    "else:\n",
    "    safe_print(\"No symbols to scrape. Run Cell 4 to load symbols.\")\n",
    "\n",
    "# --- END CELL 5 ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "961f9102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCRAPING NEWS WITH DUAL SENTIMENT ANALYSIS\n",
      "======================================================================\n",
      "This will take 1-2 minutes...\n",
      "\n",
      "\n",
      "[1/20] Fetching news for AAPL...\n",
      "  Processed 22 articles. Saved 22 new.\n",
      "\n",
      "[2/20] Fetching news for AMZN...\n",
      "  Processed 15 articles. Saved 15 new.\n",
      "\n",
      "[3/20] Fetching news for BLK...\n",
      "  Processed 1 articles. Saved 1 new.\n",
      "\n",
      "[4/20] Fetching news for ERO...\n",
      "  Processed 0 articles. Saved 0 new.\n",
      "\n",
      "[5/20] Fetching news for FXP...\n",
      "  Processed 0 articles. Saved 0 new.\n",
      "\n",
      "[6/20] Fetching news for GOOGL...\n",
      "  Processed 12 articles. Saved 12 new.\n",
      "\n",
      "[7/20] Fetching news for GXC...\n",
      "  Processed 0 articles. Saved 0 new.\n",
      "\n",
      "[8/20] Fetching news for JPM...\n",
      "  Processed 0 articles. Saved 0 new.\n",
      "\n",
      "[9/20] Fetching news for KR...\n",
      "  Processed 0 articles. Saved 0 new.\n",
      "\n",
      "[10/20] Fetching news for MDT...\n",
      "  Processed 0 articles. Saved 0 new.\n",
      "\n",
      "[11/20] Fetching news for META...\n",
      "  Processed 19 articles. Saved 19 new.\n",
      "\n",
      "[12/20] Fetching news for MSFT...\n",
      "  Processed 3 articles. Saved 3 new.\n",
      "\n",
      "[13/20] Fetching news for NVDA...\n",
      "  Processed 13 articles. Saved 13 new.\n",
      "\n",
      "[14/20] Fetching news for OXY...\n",
      "  Processed 0 articles. Saved 0 new.\n",
      "\n",
      "[15/20] Fetching news for PGJ...\n",
      "  Processed 0 articles. Saved 0 new.\n",
      "\n",
      "[16/20] Fetching news for RSP...\n",
      "  Processed 0 articles. Saved 0 new.\n",
      "\n",
      "[17/20] Fetching news for SPY...\n",
      "  Processed 0 articles. Saved 0 new.\n",
      "\n",
      "[18/20] Fetching news for TSLA...\n",
      "  Processed 24 articles. Saved 24 new.\n",
      "\n",
      "[19/20] Fetching news for VGK...\n",
      "  Processed 0 articles. Saved 0 new.\n",
      "\n",
      "[20/20] Fetching news for XPP...\n",
      "  Processed 0 articles. Saved 0 new.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 01:20:59,695 - INFO - Finished News scrape. Saved 109 new articles overall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News scraping complete!\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 6: Scrape News with Dual Sentiment ---\n",
    "\n",
    "# ============================================================================\n",
    "safe_print(\"\\n\" + \"=\"*70)\n",
    "safe_print(\"SCRAPING NEWS WITH DUAL SENTIMENT ANALYSIS\")\n",
    "safe_print(\"=\"*70)\n",
    "safe_print(\"This will take 1-2 minutes...\\n\")\n",
    "\n",
    "if symbols:\n",
    "    scraper.scrape_news(symbols, days_back=7)\n",
    "    safe_print(\"\\nNews scraping complete!\") # Removed checkmark\n",
    "else:\n",
    "    safe_print(\"No symbols to scrape. Run Cell 4 to load symbols.\")\n",
    "\n",
    "# --- END CELL 6 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b90f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 01:23:10,736 - INFO - Aggregating daily sentiment for date: 2025-10-26\n",
      "2025-10-26 01:23:10,888 - INFO - Finished aggregation for 2025-10-26. Processed 20/20 symbols.\n",
      "2025-10-26 01:23:10,890 - INFO - Aggregating daily sentiment for date: 2025-10-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "AGGREGATING DUAL SENTIMENT SCORES\n",
      "======================================================================\n",
      "Aggregating scores for today...\n",
      "\n",
      "Aggregating scores for yesterday (to ensure completeness)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 01:23:11,049 - INFO - Finished aggregation for 2025-10-25. Processed 20/20 symbols.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment aggregation complete!\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 7: Aggregate Daily Sentiment ---\n",
    "\n",
    "# ============================================================================\n",
    "safe_print(\"\\n\" + \"=\"*70)\n",
    "safe_print(\"AGGREGATING DUAL SENTIMENT SCORES\")\n",
    "safe_print(\"=\"*70)\n",
    "safe_print(\"Aggregating scores for today...\")\n",
    "\n",
    "scraper.aggregate_daily_sentiment(date_to_aggregate=datetime.now().date())\n",
    "\n",
    "safe_print(\"\\nAggregating scores for yesterday (to ensure completeness)...\")\n",
    "scraper.aggregate_daily_sentiment(date_to_aggregate=datetime.now().date() - timedelta(days=1))\n",
    "\n",
    "safe_print(\"\\nSentiment aggregation complete!\") # Removed checkmark\n",
    "\n",
    "# --- END CELL 7 ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c44b3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEXTBLOB VS FINBERT COMPARISON\n",
      "======================================================================\n",
      "\n",
      "1. OVERALL SENTIMENT STATISTICS\n",
      "----------------------------------------------------------------------\n",
      "source  total  avg_textblob  avg_finbert  avg_combined  std_textblob  std_finbert  avg_diff\n",
      "Reddit    148      0.079616    -0.047336     -0.009251       0.14692     0.378545  0.272745\n",
      "  News    109      0.072703     0.076991      0.075703       0.19819     0.668125  0.544229\n",
      "\n",
      "2. REDDIT SENTIMENT LABEL DISTRIBUTION\n",
      "----------------------------------------------------------------------\n",
      "  method    label  count  percentage\n",
      " FinBERT negative     25       16.89\n",
      " FinBERT  neutral    112       75.68\n",
      " FinBERT positive     11        7.43\n",
      "TextBlob negative      8        5.41\n",
      "TextBlob  neutral     91       61.49\n",
      "TextBlob positive     49       33.11\n",
      "\n",
      "3. TEXTBLOB-FINBERT AGREEMENT RATE\n",
      "----------------------------------------------------------------------\n",
      "source  total  agreements  agreement_rate\n",
      "Reddit    148        72.0           48.65\n",
      "  News    109        45.0           41.28\n",
      "\n",
      "4. TOP 10 STOCKS BY COMBINED SENTIMENT (Today)\n",
      "----------------------------------------------------------------------\n",
      "symbol  total_mentions  textblob_score  finbert_score  combined_score sentiment\n",
      "  AAPL               1             0.0        -0.0377         -0.0264   NEUTRAL\n",
      "\n",
      "5. STOCKS WITH HIGHEST TEXTBLOB-FINBERT DIVERGENCE (Today)\n",
      "----------------------------------------------------------------------\n",
      "symbol  total_mentions  textblob  finbert  divergence\n",
      "  AAPL               1       0.0  -0.0377      0.0377\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 8: Compare TextBlob vs FinBERT Performance ---\n",
    "\n",
    "# ============================================================================\n",
    "safe_print(\"\\n\" + \"=\"*70)\n",
    "safe_print(\"TEXTBLOB VS FINBERT COMPARISON\")\n",
    "safe_print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    conn = scraper.connect_db() # Use method to connect\n",
    "    if conn:\n",
    "        # Overall statistics\n",
    "        safe_print(\"\\n1. OVERALL SENTIMENT STATISTICS\")\n",
    "        safe_print(\"-\" * 70)\n",
    "        stats_query = \"\"\"\n",
    "        SELECT 'Reddit' as source, COUNT(*) as total, AVG(textblob_score) as avg_textblob, AVG(finbert_score) as avg_finbert, AVG(combined_score) as avg_combined,\n",
    "               STDDEV(textblob_score) as std_textblob, STDDEV(finbert_score) as std_finbert, AVG(ABS(textblob_score - finbert_score)) as avg_diff\n",
    "        FROM reddit_sentiment WHERE textblob_score IS NOT NULL AND finbert_score IS NOT NULL\n",
    "        UNION ALL\n",
    "        SELECT 'News' as source, COUNT(*), AVG(textblob_score), AVG(finbert_score), AVG(combined_score),\n",
    "               STDDEV(textblob_score), STDDEV(finbert_score), AVG(ABS(textblob_score - finbert_score))\n",
    "        FROM news_sentiment WHERE textblob_score IS NOT NULL AND finbert_score IS NOT NULL\n",
    "        \"\"\"\n",
    "        stats_df = pd.read_sql(stats_query, conn)\n",
    "        safe_print(stats_df.to_string(index=False))\n",
    "\n",
    "        # Sentiment distribution comparison (Reddit only)\n",
    "        safe_print(\"\\n2. REDDIT SENTIMENT LABEL DISTRIBUTION\")\n",
    "        safe_print(\"-\" * 70)\n",
    "        label_query = \"\"\"\n",
    "        SELECT 'TextBlob' as method, textblob_label as label, COUNT(*) as count, ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "        FROM reddit_sentiment GROUP BY textblob_label\n",
    "        UNION ALL\n",
    "        SELECT 'FinBERT' as method, finbert_label as label, COUNT(*) as count, ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "        FROM reddit_sentiment GROUP BY finbert_label ORDER BY method, label\n",
    "        \"\"\"\n",
    "        label_df = pd.read_sql(label_query, conn)\n",
    "        safe_print(label_df.to_string(index=False))\n",
    "\n",
    "        # Agreement rate\n",
    "        safe_print(\"\\n3. TEXTBLOB-FINBERT AGREEMENT RATE\")\n",
    "        safe_print(\"-\" * 70)\n",
    "        agreement_query = \"\"\"\n",
    "        SELECT 'Reddit' as source, COUNT(*) as total, SUM(CASE WHEN textblob_label = finbert_label THEN 1 ELSE 0 END) as agreements,\n",
    "               ROUND(SUM(CASE WHEN textblob_label = finbert_label THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as agreement_rate\n",
    "        FROM reddit_sentiment\n",
    "        UNION ALL\n",
    "        SELECT 'News' as source, COUNT(*), SUM(CASE WHEN textblob_label = finbert_label THEN 1 ELSE 0 END),\n",
    "               ROUND(SUM(CASE WHEN textblob_label = finbert_label THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2)\n",
    "        FROM news_sentiment\n",
    "        \"\"\"\n",
    "        agreement_df = pd.read_sql(agreement_query, conn)\n",
    "        safe_print(agreement_df.to_string(index=False))\n",
    "\n",
    "        # Top stocks by sentiment\n",
    "        safe_print(\"\\n4. TOP 10 STOCKS BY COMBINED SENTIMENT (Today)\")\n",
    "        safe_print(\"-\" * 70)\n",
    "        top_stocks_query = \"\"\"\n",
    "        SELECT symbol, total_mentions, ROUND(overall_textblob, 4) as textblob_score, ROUND(overall_finbert, 4) as finbert_score,\n",
    "               ROUND(overall_combined, 4) as combined_score,\n",
    "               CASE WHEN overall_combined > 0.1 THEN 'POSITIVE' WHEN overall_combined < -0.1 THEN 'NEGATIVE' ELSE 'NEUTRAL' END as sentiment\n",
    "        FROM daily_sentiment WHERE date = CURDATE() AND total_mentions > 0 ORDER BY total_mentions DESC LIMIT 10\n",
    "        \"\"\"\n",
    "        top_stocks_df = pd.read_sql(top_stocks_query, conn)\n",
    "        if not top_stocks_df.empty:\n",
    "            safe_print(top_stocks_df.to_string(index=False))\n",
    "        else:\n",
    "            safe_print(\"No sentiment data for today yet.\")\n",
    "\n",
    "        # Divergence analysis\n",
    "        safe_print(\"\\n5. STOCKS WITH HIGHEST TEXTBLOB-FINBERT DIVERGENCE (Today)\")\n",
    "        safe_print(\"-\" * 70)\n",
    "        divergence_query = \"\"\"\n",
    "        SELECT symbol, total_mentions, ROUND(overall_textblob, 4) as textblob, ROUND(overall_finbert, 4) as finbert,\n",
    "               ROUND(ABS(overall_textblob - overall_finbert), 4) as divergence\n",
    "        FROM daily_sentiment WHERE date = CURDATE() AND total_mentions > 0 ORDER BY ABS(overall_textblob - overall_finbert) DESC LIMIT 10\n",
    "        \"\"\"\n",
    "        divergence_df = pd.read_sql(divergence_query, conn)\n",
    "        if not divergence_df.empty:\n",
    "            safe_print(divergence_df.to_string(index=False))\n",
    "        else:\n",
    "            safe_print(\"No sentiment data for today yet.\")\n",
    "\n",
    "        conn.close()\n",
    "    else:\n",
    "         safe_print(\"Could not connect to database for analysis.\")\n",
    "\n",
    "except Exception as e:\n",
    "    safe_print(f\"An error occurred during analysis: {e}\")\n",
    "    safe_print(\"Please ensure your database is running and tables were created.\")\n",
    "\n",
    "# --- END CELL 8 ---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
