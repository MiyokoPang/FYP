{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b57e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import praw\n",
    "import requests\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import logging\n",
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32681d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "reddit_config = {\n",
    "    'client_id': '6SD-D4KilOopu4O6m9R9VA',\n",
    "    'client_secret': 'mv-Z3_vFTC7WApSWvtdxUX0MKDq3QQ',\n",
    "    'user_agent': 'Trading Sentiment Bot by Delicious_Divide6891'\n",
    "}\n",
    "\n",
    "news_api_key = '73e9447f080543c3885ec7803f705101'\n",
    "\n",
    "db_config = {\n",
    "    'host': '127.0.0.1',\n",
    "    'user': 'root',\n",
    "    'password': '',\n",
    "    'database': 'trading_system'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentScraper:\n",
    "    def __init__(self, reddit_config, news_api_key, db_config):\n",
    "        self.reddit_config = reddit_config\n",
    "        self.news_api_key = news_api_key\n",
    "        self.db_config = db_config\n",
    "        self.setup_logging()\n",
    "        self.reddit = None\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('sentiment_scraper.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def connect_db(self):\n",
    "        \"\"\"Connect to MySQL database\"\"\"\n",
    "        try:\n",
    "            conn = mysql.connector.connect(**self.db_config)\n",
    "            return conn\n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Database connection error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_sentiment_tables(self):\n",
    "        \"\"\"Create tables for sentiment data\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Reddit sentiment data - NO URL COLUMN\n",
    "        reddit_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS reddit_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            symbol VARCHAR(10),\n",
    "            subreddit VARCHAR(50),\n",
    "            post_id VARCHAR(20) UNIQUE,\n",
    "            title TEXT,\n",
    "            selftext TEXT,\n",
    "            score INT,\n",
    "            num_comments INT,\n",
    "            created_utc TIMESTAMP,\n",
    "            sentiment_score DECIMAL(5,4),\n",
    "            sentiment_label VARCHAR(20),\n",
    "            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            INDEX idx_symbol (symbol),\n",
    "            INDEX idx_created (created_utc)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # News sentiment data - NO URL COLUMN\n",
    "        news_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS news_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            symbol VARCHAR(10),\n",
    "            source VARCHAR(100),\n",
    "            title TEXT,\n",
    "            description TEXT,\n",
    "            published_at TIMESTAMP,\n",
    "            sentiment_score DECIMAL(5,4),\n",
    "            sentiment_label VARCHAR(20),\n",
    "            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            INDEX idx_symbol (symbol),\n",
    "            INDEX idx_published (published_at)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # Aggregated daily sentiment\n",
    "        daily_sentiment_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS daily_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            symbol VARCHAR(10),\n",
    "            date DATE,\n",
    "            reddit_avg_sentiment DECIMAL(5,4),\n",
    "            reddit_post_count INT DEFAULT 0,\n",
    "            news_avg_sentiment DECIMAL(5,4),\n",
    "            news_article_count INT DEFAULT 0,\n",
    "            combined_sentiment DECIMAL(5,4),\n",
    "            total_mentions INT DEFAULT 0,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE KEY unique_symbol_date (symbol, date)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        tables = [reddit_table, news_table, daily_sentiment_table]\n",
    "        \n",
    "        try:\n",
    "            for table in tables:\n",
    "                cursor.execute(table)\n",
    "            conn.commit()\n",
    "            self.logger.info(\"Sentiment tables created successfully\")\n",
    "            return True\n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Error creating sentiment tables: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "    \n",
    "    def init_reddit(self):\n",
    "        \"\"\"Initialize Reddit API connection\"\"\"\n",
    "        try:\n",
    "            self.reddit = praw.Reddit(\n",
    "                client_id=self.reddit_config['client_id'],\n",
    "                client_secret=self.reddit_config['client_secret'],\n",
    "                user_agent=self.reddit_config['user_agent']\n",
    "            )\n",
    "            self.logger.info(\"Reddit API initialized successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing Reddit API: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment using TextBlob\n",
    "        Returns: (sentiment_score, sentiment_label)\n",
    "        Score: -1 (very negative) to +1 (very positive)\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return 0.0, 'neutral'\n",
    "        \n",
    "        try:\n",
    "            blob = TextBlob(text)\n",
    "            polarity = blob.sentiment.polarity\n",
    "            \n",
    "            # Classify sentiment\n",
    "            if polarity > 0.1:\n",
    "                label = 'positive'\n",
    "            elif polarity < -0.1:\n",
    "                label = 'negative'\n",
    "            else:\n",
    "                label = 'neutral'\n",
    "            \n",
    "            return round(polarity, 4), label\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing sentiment: {e}\")\n",
    "            return 0.0, 'neutral'\n",
    "    \n",
    "    def scrape_reddit(self, symbols, subreddits=['stocks', 'investing', 'wallstreetbets', 'SecurityAnalysis'], \n",
    "                     limit=100, time_filter='week'):\n",
    "        if not self.reddit:\n",
    "            if not self.init_reddit():\n",
    "                return False\n",
    "        \n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        total_posts = 0\n",
    "        \n",
    "        for idx, symbol in enumerate(symbols, 1):\n",
    "            print(f\"\\n[{idx}/{len(symbols)}] Processing {symbol}...\")\n",
    "            for subreddit_name in subreddits:\n",
    "                try:\n",
    "                    print(f\"  → Searching r/{subreddit_name}...\", end='')\n",
    "                    subreddit = self.reddit.subreddit(subreddit_name)\n",
    "                    \n",
    "                    # Search for stock symbol\n",
    "                    search_query = f\"${symbol} OR {symbol}\"\n",
    "                    \n",
    "                    post_count = 0\n",
    "                    for post in subreddit.search(search_query, time_filter=time_filter, limit=limit):\n",
    "                        post_count += 1\n",
    "                        # Combine title and selftext for sentiment analysis\n",
    "                        full_text = f\"{post.title} {post.selftext}\"\n",
    "                        sentiment_score, sentiment_label = self.analyze_sentiment(full_text)\n",
    "                        \n",
    "                        try:\n",
    "                            query = \"\"\"\n",
    "                            INSERT IGNORE INTO reddit_sentiment \n",
    "                            (symbol, subreddit, post_id, title, selftext, score, num_comments, \n",
    "                             created_utc, sentiment_score, sentiment_label)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                            \"\"\"\n",
    "                            \n",
    "                            values = (\n",
    "                                symbol,\n",
    "                                subreddit_name,\n",
    "                                post.id,\n",
    "                                post.title[:500],  # Limit length\n",
    "                                post.selftext[:1000] if post.selftext else '',\n",
    "                                post.score,\n",
    "                                post.num_comments,\n",
    "                                datetime.fromtimestamp(post.created_utc),\n",
    "                                sentiment_score,\n",
    "                                sentiment_label\n",
    "                            )\n",
    "                            \n",
    "                            cursor.execute(query, values)\n",
    "                            total_posts += 1\n",
    "                            \n",
    "                        except mysql.connector.Error as e:\n",
    "                            if \"Duplicate entry\" not in str(e):\n",
    "                                self.logger.error(f\"Error inserting Reddit post: {e}\")\n",
    "                    \n",
    "                    print(f\" {post_count} posts found\")\n",
    "                    # Rate limiting - be nice to Reddit\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error scraping r/{subreddit_name} for {symbol}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        self.logger.info(f\"Scraped {total_posts} Reddit posts\")\n",
    "        return True\n",
    "    \n",
    "    def scrape_news(self, symbols, days_back=7):\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        total_articles = 0\n",
    "        \n",
    "        # Calculate date range\n",
    "        to_date = datetime.now()\n",
    "        from_date = to_date - timedelta(days=days_back)\n",
    "        \n",
    "        for idx, symbol in enumerate(symbols, 1):\n",
    "            print(f\"\\n[{idx}/{len(symbols)}] Fetching news for {symbol}...\")\n",
    "            try:\n",
    "                # NewsAPI endpoint\n",
    "                url = 'https://newsapi.org/v2/everything'\n",
    "                \n",
    "                params = {\n",
    "                    'q': f\"{symbol} stock OR {symbol} shares\",\n",
    "                    'from': from_date.strftime('%Y-%m-%d'),\n",
    "                    'to': to_date.strftime('%Y-%m-%d'),\n",
    "                    'language': 'en',\n",
    "                    'sortBy': 'publishedAt',\n",
    "                    'apiKey': self.news_api_key,\n",
    "                    'pageSize': 100\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, params=params)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    articles = data.get('articles', [])\n",
    "                    \n",
    "                    for article in articles:\n",
    "                        # Analyze sentiment from title + description\n",
    "                        text = f\"{article.get('title', '')} {article.get('description', '')}\"\n",
    "                        sentiment_score, sentiment_label = self.analyze_sentiment(text)\n",
    "                        \n",
    "                        try:\n",
    "                            query = \"\"\"\n",
    "                            INSERT IGNORE INTO news_sentiment\n",
    "                            (symbol, source, title, description, published_at, \n",
    "                             sentiment_score, sentiment_label)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                            \"\"\"\n",
    "                            \n",
    "                            # Parse published date\n",
    "                            published = article.get('publishedAt', '')\n",
    "                            if published:\n",
    "                                published_dt = datetime.strptime(published, '%Y-%m-%dT%H:%M:%SZ')\n",
    "                            else:\n",
    "                                published_dt = datetime.now()\n",
    "                            \n",
    "                            values = (\n",
    "                                symbol,\n",
    "                                article.get('source', {}).get('name', 'Unknown')[:100],\n",
    "                                article.get('title', '')[:500],\n",
    "                                article.get('description', '')[:1000],\n",
    "                                published_dt,\n",
    "                                sentiment_score,\n",
    "                                sentiment_label\n",
    "                            )\n",
    "                            \n",
    "                            cursor.execute(query, values)\n",
    "                            total_articles += 1\n",
    "                            \n",
    "                        except mysql.connector.Error as e:\n",
    "                            if \"Duplicate entry\" not in str(e):\n",
    "                                self.logger.error(f\"Error inserting news article: {e}\")\n",
    "                    \n",
    "                    print(f\"  ✓ Found {len(articles)} articles\")\n",
    "                    \n",
    "                elif response.status_code == 426:\n",
    "                    self.logger.warning(\"NewsAPI rate limit reached. Upgrade plan or wait.\")\n",
    "                    break\n",
    "                else:\n",
    "                    self.logger.error(f\"NewsAPI error for {symbol}: {response.status_code}\")\n",
    "                \n",
    "                # Rate limiting - NewsAPI allows 1 request per second on free tier\n",
    "                time.sleep(1.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error scraping news for {symbol}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        self.logger.info(f\"Scraped {total_articles} news articles\")\n",
    "        return True\n",
    "    \n",
    "    def aggregate_daily_sentiment(self, date=None):\n",
    "        \"\"\"\n",
    "        Aggregate sentiment data by symbol and date\n",
    "        date: specific date to aggregate (default: today)\n",
    "        \"\"\"\n",
    "        if date is None:\n",
    "            date = datetime.now().date()\n",
    "        \n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get list of symbols\n",
    "        cursor.execute(\"SELECT DISTINCT symbol FROM stocks\")\n",
    "        symbols = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            try:\n",
    "                # Aggregate Reddit sentiment\n",
    "                reddit_query = \"\"\"\n",
    "                SELECT \n",
    "                    AVG(sentiment_score) as avg_sentiment,\n",
    "                    COUNT(*) as post_count\n",
    "                FROM reddit_sentiment\n",
    "                WHERE symbol = %s AND DATE(created_utc) = %s\n",
    "                \"\"\"\n",
    "                cursor.execute(reddit_query, (symbol, date))\n",
    "                reddit_result = cursor.fetchone()\n",
    "                reddit_avg = reddit_result[0] if reddit_result[0] else 0\n",
    "                reddit_count = reddit_result[1] if reddit_result[1] else 0\n",
    "                \n",
    "                # Aggregate News sentiment\n",
    "                news_query = \"\"\"\n",
    "                SELECT \n",
    "                    AVG(sentiment_score) as avg_sentiment,\n",
    "                    COUNT(*) as article_count\n",
    "                FROM news_sentiment\n",
    "                WHERE symbol = %s AND DATE(published_at) = %s\n",
    "                \"\"\"\n",
    "                cursor.execute(news_query, (symbol, date))\n",
    "                news_result = cursor.fetchone()\n",
    "                news_avg = news_result[0] if news_result[0] else 0\n",
    "                news_count = news_result[1] if news_result[1] else 0\n",
    "                \n",
    "                # Calculate combined sentiment (weighted by count)\n",
    "                total_count = reddit_count + news_count\n",
    "                if total_count > 0:\n",
    "                    combined = ((reddit_avg * reddit_count) + (news_avg * news_count)) / total_count\n",
    "                else:\n",
    "                    combined = 0\n",
    "                \n",
    "                # Insert or update aggregated data\n",
    "                insert_query = \"\"\"\n",
    "                INSERT INTO daily_sentiment \n",
    "                (symbol, date, reddit_avg_sentiment, reddit_post_count, \n",
    "                 news_avg_sentiment, news_article_count, combined_sentiment, total_mentions)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON DUPLICATE KEY UPDATE\n",
    "                    reddit_avg_sentiment = VALUES(reddit_avg_sentiment),\n",
    "                    reddit_post_count = VALUES(reddit_post_count),\n",
    "                    news_avg_sentiment = VALUES(news_avg_sentiment),\n",
    "                    news_article_count = VALUES(news_article_count),\n",
    "                    combined_sentiment = VALUES(combined_sentiment),\n",
    "                    total_mentions = VALUES(total_mentions)\n",
    "                \"\"\"\n",
    "                \n",
    "                cursor.execute(insert_query, (\n",
    "                    symbol, date, reddit_avg, reddit_count,\n",
    "                    news_avg, news_count, combined, total_count\n",
    "                ))\n",
    "                \n",
    "            except mysql.connector.Error as e:\n",
    "                self.logger.error(f\"Error aggregating sentiment for {symbol}: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        self.logger.info(f\"Aggregated sentiment data for {len(symbols)} symbols on {date}\")\n",
    "        return True\n",
    "    \n",
    "    def get_stock_symbols(self):\n",
    "        \"\"\"Get all tracked stock symbols from database\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return []\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT symbol FROM stocks\")\n",
    "        symbols = [row[0] for row in cursor.fetchall()]\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ec12f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 19:11:38,641 - INFO - package: mysql.connector.plugins\n",
      "2025-10-04 19:11:38,642 - INFO - plugin_name: caching_sha2_password\n",
      "2025-10-04 19:11:38,643 - INFO - AUTHENTICATION_PLUGIN_CLASS: MySQLCachingSHA2PasswordAuthPlugin\n",
      "2025-10-04 19:11:38,644 - INFO - Sentiment tables created successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete! Tracking 24 stocks: ['AAPL', 'AMD', 'AMZN', 'BLK', 'CHN', 'ERO', 'FXP', 'GOOGL', 'GXC', 'JPM', 'KR', 'MDT', 'META', 'MSFT', 'NFLX', 'NVDA', 'OXY', 'PGJ', 'RSP', 'SPY', 'TSLA', 'VGK', 'XPP', 'YINN']\n"
     ]
    }
   ],
   "source": [
    "# Initialize scraper\n",
    "scraper = SentimentScraper(reddit_config, news_api_key, db_config)\n",
    "\n",
    "# Create sentiment tables\n",
    "scraper.create_sentiment_tables()\n",
    "\n",
    "# Get stock symbols from database\n",
    "symbols = scraper.get_stock_symbols()\n",
    "print(f\"✓ Setup complete! Tracking {len(symbols)} stocks: {symbols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "128f8a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Reddit scraping...\n",
      "\n",
      "[1/24] Processing AAPL...\n",
      "  → Searching r/stocks... 1 posts found\n",
      "  → Searching r/investing... 2 posts found\n",
      "  → Searching r/wallstreetbets... 5 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[2/24] Processing AMD...\n",
      "  → Searching r/stocks... 2 posts found\n",
      "  → Searching r/investing... 3 posts found\n",
      "  → Searching r/wallstreetbets... 7 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[3/24] Processing AMZN...\n",
      "  → Searching r/stocks... 2 posts found\n",
      "  → Searching r/investing... 4 posts found\n",
      "  → Searching r/wallstreetbets... 4 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[4/24] Processing BLK...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[5/24] Processing CHN...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[6/24] Processing ERO...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[7/24] Processing FXP...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[8/24] Processing GOOGL...\n",
      "  → Searching r/stocks... 28 posts found\n",
      "  → Searching r/investing... 16 posts found\n",
      "  → Searching r/wallstreetbets... 10 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[9/24] Processing GXC...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[10/24] Processing JPM...\n",
      "  → Searching r/stocks... 1 posts found\n",
      "  → Searching r/investing... 4 posts found\n",
      "  → Searching r/wallstreetbets... 1 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[11/24] Processing KR...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[12/24] Processing MDT...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 1 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[13/24] Processing META...\n",
      "  → Searching r/stocks... 7 posts found\n",
      "  → Searching r/investing... 3 posts found\n",
      "  → Searching r/wallstreetbets... 8 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[14/24] Processing MSFT...\n",
      "  → Searching r/stocks... 3 posts found\n",
      "  → Searching r/investing... 2 posts found\n",
      "  → Searching r/wallstreetbets... 4 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[15/24] Processing NFLX...\n",
      "  → Searching r/stocks... 1 posts found\n",
      "  → Searching r/investing... 1 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[16/24] Processing NVDA...\n",
      "  → Searching r/stocks... 9 posts found\n",
      "  → Searching r/investing... 10 posts found\n",
      "  → Searching r/wallstreetbets... 9 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[17/24] Processing OXY...\n",
      "  → Searching r/stocks... 2 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[18/24] Processing PGJ...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[19/24] Processing RSP...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[20/24] Processing SPY...\n",
      "  → Searching r/stocks... 9 posts found\n",
      "  → Searching r/investing... 3 posts found\n",
      "  → Searching r/wallstreetbets... 27 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[21/24] Processing TSLA...\n",
      "  → Searching r/stocks... 7 posts found\n",
      "  → Searching r/investing... 1 posts found\n",
      "  → Searching r/wallstreetbets... 3 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[22/24] Processing VGK...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[23/24] Processing XPP...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[24/24] Processing YINN...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 1 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 19:22:29,225 - INFO - Scraped 201 Reddit posts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Reddit scraping complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Reddit scraping...\")\n",
    "scraper.scrape_reddit(symbols, limit=50, time_filter='week')\n",
    "print(\"\\n✓ Reddit scraping complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4998b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting news scraping...\n",
      "This will take 1-2 minutes.\n",
      "\n",
      "\n",
      "[1/24] Fetching news for AAPL...\n",
      "  ✓ Found 92 articles\n",
      "\n",
      "[2/24] Fetching news for AMD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 19:16:48,686 - ERROR - Error scraping news for AMD: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/24] Fetching news for AMZN...\n",
      "  ✓ Found 97 articles\n",
      "\n",
      "[4/24] Fetching news for BLK...\n",
      "  ✓ Found 10 articles\n",
      "\n",
      "[5/24] Fetching news for CHN...\n",
      "  ✓ Found 3 articles\n",
      "\n",
      "[6/24] Fetching news for ERO...\n",
      "  ✓ Found 5 articles\n",
      "\n",
      "[7/24] Fetching news for FXP...\n",
      "  ✓ Found 0 articles\n",
      "\n",
      "[8/24] Fetching news for GOOGL...\n",
      "  ✓ Found 100 articles\n",
      "\n",
      "[9/24] Fetching news for GXC...\n",
      "  ✓ Found 2 articles\n",
      "\n",
      "[10/24] Fetching news for JPM...\n",
      "  ✓ Found 100 articles\n",
      "\n",
      "[11/24] Fetching news for KR...\n",
      "  ✓ Found 6 articles\n",
      "\n",
      "[12/24] Fetching news for MDT...\n",
      "  ✓ Found 9 articles\n",
      "\n",
      "[13/24] Fetching news for META...\n",
      "  ✓ Found 99 articles\n",
      "\n",
      "[14/24] Fetching news for MSFT...\n",
      "  ✓ Found 89 articles\n",
      "\n",
      "[15/24] Fetching news for NFLX...\n",
      "  ✓ Found 6 articles\n",
      "\n",
      "[16/24] Fetching news for NVDA...\n",
      "  ✓ Found 100 articles\n",
      "\n",
      "[17/24] Fetching news for OXY...\n",
      "  ✓ Found 24 articles\n",
      "\n",
      "[18/24] Fetching news for PGJ...\n",
      "  ✓ Found 0 articles\n",
      "\n",
      "[19/24] Fetching news for RSP...\n",
      "  ✓ Found 3 articles\n",
      "\n",
      "[20/24] Fetching news for SPY...\n",
      "  ✓ Found 30 articles\n",
      "\n",
      "[21/24] Fetching news for TSLA...\n",
      "  ✓ Found 66 articles\n",
      "\n",
      "[22/24] Fetching news for VGK...\n",
      "  ✓ Found 5 articles\n",
      "\n",
      "[23/24] Fetching news for XPP...\n",
      "  ✓ Found 0 articles\n",
      "\n",
      "[24/24] Fetching news for YINN...\n",
      "  ✓ Found 0 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 19:17:34,788 - INFO - Scraped 856 news articles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ News scraping complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting news scraping...\")\n",
    "print(\"This will take 1-2 minutes.\\n\")\n",
    "\n",
    "scraper.scrape_news(symbols, days_back=7)\n",
    "\n",
    "print(\"\\n✓ News scraping complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49267b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 19:22:43,072 - INFO - Aggregated sentiment data for 24 symbols on 2025-10-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating daily sentiment scores...\n",
      "✓ Sentiment aggregation complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Aggregating daily sentiment scores...\")\n",
    "\n",
    "scraper.aggregate_daily_sentiment()\n",
    "\n",
    "print(\"✓ Sentiment aggregation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ab25017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit posts collected: 150\n",
      "News articles collected: 856\n",
      "\n",
      "============================================================\n",
      "TOP 10 STOCKS BY TOTAL MENTIONS:\n",
      "============================================================\n",
      "GOOGL  | Sentiment: +0.116 | Mentions:   6 (Reddit: 6, News: 0)\n",
      "SPY    | Sentiment: -0.052 | Mentions:   6 (Reddit: 6, News: 0)\n",
      "AAPL   | Sentiment: +0.088 | Mentions:   3 (Reddit: 3, News: 0)\n",
      "AMZN   | Sentiment: -0.117 | Mentions:   2 (Reddit: 2, News: 0)\n",
      "META   | Sentiment: +0.154 | Mentions:   2 (Reddit: 2, News: 0)\n",
      "AMD    | Sentiment: +0.219 | Mentions:   1 (Reddit: 1, News: 0)\n",
      "JPM    | Sentiment: +0.107 | Mentions:   1 (Reddit: 1, News: 0)\n",
      "MSFT   | Sentiment: +0.109 | Mentions:   1 (Reddit: 1, News: 0)\n",
      "NVDA   | Sentiment: +0.050 | Mentions:   1 (Reddit: 1, News: 0)\n",
      "BLK    | Sentiment: +0.000 | Mentions:   0 (Reddit: 0, News: 0)\n",
      "\n",
      "✓ All sentiment data collection complete!\n",
      "Check your database tables: reddit_sentiment, news_sentiment, daily_sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18kyu\\AppData\\Local\\Temp\\ipykernel_22652\\613416929.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  reddit_df = pd.read_sql(\"SELECT COUNT(*) as total FROM reddit_sentiment\", conn)\n",
      "C:\\Users\\18kyu\\AppData\\Local\\Temp\\ipykernel_22652\\613416929.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  news_df = pd.read_sql(\"SELECT COUNT(*) as total FROM news_sentiment\", conn)\n",
      "C:\\Users\\18kyu\\AppData\\Local\\Temp\\ipykernel_22652\\613416929.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sentiment_df = pd.read_sql(\"\"\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "\n",
    "# Reddit posts count\n",
    "reddit_df = pd.read_sql(\"SELECT COUNT(*) as total FROM reddit_sentiment\", conn)\n",
    "print(f\"Reddit posts collected: {reddit_df['total'][0]}\")\n",
    "\n",
    "# News articles count\n",
    "news_df = pd.read_sql(\"SELECT COUNT(*) as total FROM news_sentiment\", conn)\n",
    "print(f\"News articles collected: {news_df['total'][0]}\")\n",
    "\n",
    "# Top stocks by sentiment mentions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 STOCKS BY TOTAL MENTIONS:\")\n",
    "print(\"=\"*60)\n",
    "sentiment_df = pd.read_sql(\"\"\"\n",
    "    SELECT symbol, combined_sentiment, total_mentions, \n",
    "           reddit_post_count, news_article_count\n",
    "    FROM daily_sentiment \n",
    "    WHERE date = CURDATE()\n",
    "    ORDER BY total_mentions DESC \n",
    "    LIMIT 10\n",
    "\"\"\", conn)\n",
    "\n",
    "for idx, row in sentiment_df.iterrows():\n",
    "    print(f\"{row['symbol']:6s} | Sentiment: {row['combined_sentiment']:+.3f} | \"\n",
    "          f\"Mentions: {row['total_mentions']:3d} \"\n",
    "          f\"(Reddit: {row['reddit_post_count']}, News: {row['news_article_count']})\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n✓ All sentiment data collection complete!\")\n",
    "print(\"Check your database tables: reddit_sentiment, news_sentiment, daily_sentiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
