{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import requests\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import logging\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "class SentimentScraper:\n",
    "    def __init__(self, reddit_config, news_api_key, db_config):\n",
    "        self.reddit_config = reddit_config\n",
    "        self.news_api_key = news_api_key\n",
    "        self.db_config = db_config\n",
    "        self.setup_logging()\n",
    "        self.reddit = None\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('sentiment_scraper.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def connect_db(self):\n",
    "        \"\"\"Connect to MySQL database\"\"\"\n",
    "        try:\n",
    "            conn = mysql.connector.connect(**self.db_config)\n",
    "            return conn\n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Database connection error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_sentiment_tables(self):\n",
    "        \"\"\"Create tables for sentiment data\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Reddit sentiment data\n",
    "        reddit_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS reddit_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            symbol VARCHAR(10),\n",
    "            subreddit VARCHAR(50),\n",
    "            post_id VARCHAR(20) UNIQUE,\n",
    "            title TEXT,\n",
    "            selftext TEXT,\n",
    "            score INT,\n",
    "            num_comments INT,\n",
    "            created_utc TIMESTAMP,\n",
    "            url TEXT,\n",
    "            sentiment_score DECIMAL(5,4),\n",
    "            sentiment_label VARCHAR(20),\n",
    "            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            INDEX idx_symbol (symbol),\n",
    "            INDEX idx_created (created_utc)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # News sentiment data\n",
    "        news_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS news_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            symbol VARCHAR(10),\n",
    "            source VARCHAR(100),\n",
    "            title TEXT,\n",
    "            description TEXT,\n",
    "            url TEXT UNIQUE,\n",
    "            published_at TIMESTAMP,\n",
    "            sentiment_score DECIMAL(5,4),\n",
    "            sentiment_label VARCHAR(20),\n",
    "            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            INDEX idx_symbol (symbol),\n",
    "            INDEX idx_published (published_at)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # Aggregated daily sentiment\n",
    "        daily_sentiment_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS daily_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            symbol VARCHAR(10),\n",
    "            date DATE,\n",
    "            reddit_avg_sentiment DECIMAL(5,4),\n",
    "            reddit_post_count INT DEFAULT 0,\n",
    "            news_avg_sentiment DECIMAL(5,4),\n",
    "            news_article_count INT DEFAULT 0,\n",
    "            combined_sentiment DECIMAL(5,4),\n",
    "            total_mentions INT DEFAULT 0,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE KEY unique_symbol_date (symbol, date)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        tables = [reddit_table, news_table, daily_sentiment_table]\n",
    "        \n",
    "        try:\n",
    "            for table in tables:\n",
    "                cursor.execute(table)\n",
    "            conn.commit()\n",
    "            self.logger.info(\"Sentiment tables created successfully\")\n",
    "            return True\n",
    "        except mysql.connector.Error as e:\n",
    "            self.logger.error(f\"Error creating sentiment tables: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "    \n",
    "    def init_reddit(self):\n",
    "        \"\"\"Initialize Reddit API connection\"\"\"\n",
    "        try:\n",
    "            self.reddit = praw.Reddit(\n",
    "                client_id=self.reddit_config['client_id'],\n",
    "                client_secret=self.reddit_config['client_secret'],\n",
    "                user_agent=self.reddit_config['user_agent']\n",
    "            )\n",
    "            self.logger.info(\"Reddit API initialized successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing Reddit API: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment using TextBlob\n",
    "        Returns: (sentiment_score, sentiment_label)\n",
    "        Score: -1 (very negative) to +1 (very positive)\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return 0.0, 'neutral'\n",
    "        \n",
    "        try:\n",
    "            blob = TextBlob(text)\n",
    "            polarity = blob.sentiment.polarity\n",
    "            \n",
    "            # Classify sentiment\n",
    "            if polarity > 0.1:\n",
    "                label = 'positive'\n",
    "            elif polarity < -0.1:\n",
    "                label = 'negative'\n",
    "            else:\n",
    "                label = 'neutral'\n",
    "            \n",
    "            return round(polarity, 4), label\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing sentiment: {e}\")\n",
    "            return 0.0, 'neutral'\n",
    "    \n",
    "    def scrape_reddit(self, symbols, subreddits=['stocks', 'investing', 'wallstreetbets', 'SecurityAnalysis'], \n",
    "                     limit=100, time_filter='week'):\n",
    "        \"\"\"\n",
    "        Scrape Reddit for stock mentions\n",
    "        symbols: list of stock symbols to search for\n",
    "        subreddits: list of subreddits to search\n",
    "        limit: number of posts per search\n",
    "        time_filter: 'day', 'week', 'month'\n",
    "        \"\"\"\n",
    "        if not self.reddit:\n",
    "            if not self.init_reddit():\n",
    "                return False\n",
    "        \n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        total_posts = 0\n",
    "        \n",
    "        for idx, symbol in enumerate(symbols, 1):\n",
    "            print(f\"\\n[{idx}/{len(symbols)}] Processing {symbol}...\")\n",
    "            for subreddit_name in subreddits:\n",
    "                try:\n",
    "                    print(f\"  → Searching r/{subreddit_name}...\", end='')\n",
    "                    subreddit = self.reddit.subreddit(subreddit_name)\n",
    "                    \n",
    "                    # Search for stock symbol\n",
    "                    search_query = f\"${symbol} OR {symbol}\"\n",
    "                    \n",
    "                    post_count = 0\n",
    "                    for post in subreddit.search(search_query, time_filter=time_filter, limit=limit):\n",
    "                        post_count += 1\n",
    "                        # Combine title and selftext for sentiment analysis\n",
    "                        full_text = f\"{post.title} {post.selftext}\"\n",
    "                        sentiment_score, sentiment_label = self.analyze_sentiment(full_text)\n",
    "                        \n",
    "                        try:\n",
    "                            query = \"\"\"\n",
    "                            INSERT IGNORE INTO reddit_sentiment \n",
    "                            (symbol, subreddit, post_id, title, selftext, score, num_comments, \n",
    "                             created_utc, url, sentiment_score, sentiment_label)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                            \"\"\"\n",
    "                            \n",
    "                            values = (\n",
    "                                symbol,\n",
    "                                subreddit_name,\n",
    "                                post.id,\n",
    "                                post.title[:500],  # Limit length\n",
    "                                post.selftext[:1000] if post.selftext else '',\n",
    "                                post.score,\n",
    "                                post.num_comments,\n",
    "                                datetime.fromtimestamp(post.created_utc),\n",
    "                                post.url,\n",
    "                                sentiment_score,\n",
    "                                sentiment_label\n",
    "                            )\n",
    "                            \n",
    "                            cursor.execute(query, values)\n",
    "                            total_posts += 1\n",
    "                            \n",
    "                        except mysql.connector.Error as e:\n",
    "                            if \"Duplicate entry\" not in str(e):\n",
    "                                self.logger.error(f\"Error inserting Reddit post: {e}\")\n",
    "                    \n",
    "                    print(f\" {post_count} posts found\")\n",
    "                    # Rate limiting - be nice to Reddit\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error scraping r/{subreddit_name} for {symbol}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        self.logger.info(f\"Scraped {total_posts} Reddit posts\")\n",
    "        return True\n",
    "    \n",
    "    def scrape_news(self, symbols, days_back=7):\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        total_articles = 0\n",
    "        \n",
    "        # Calculate date range\n",
    "        to_date = datetime.now()\n",
    "        from_date = to_date - timedelta(days=days_back)\n",
    "        \n",
    "        for idx, symbol in enumerate(symbols, 1):\n",
    "            print(f\"\\n[{idx}/{len(symbols)}] Fetching news for {symbol}...\")\n",
    "            try:\n",
    "                # NewsAPI endpoint\n",
    "                url = 'https://newsapi.org/v2/everything'\n",
    "                \n",
    "                params = {\n",
    "                    'q': f\"{symbol} stock OR {symbol} shares\",\n",
    "                    'from': from_date.strftime('%Y-%m-%d'),\n",
    "                    'to': to_date.strftime('%Y-%m-%d'),\n",
    "                    'language': 'en',\n",
    "                    'sortBy': 'publishedAt',\n",
    "                    'apiKey': self.news_api_key,\n",
    "                    'pageSize': 100\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, params=params)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    articles = data.get('articles', [])\n",
    "                    \n",
    "                    for article in articles:\n",
    "                        # Analyze sentiment from title + description\n",
    "                        text = f\"{article.get('title', '')} {article.get('description', '')}\"\n",
    "                        sentiment_score, sentiment_label = self.analyze_sentiment(text)\n",
    "                        \n",
    "                        try:\n",
    "                            query = \"\"\"\n",
    "                            INSERT IGNORE INTO news_sentiment\n",
    "                            (symbol, source, title, description, url, published_at, \n",
    "                             sentiment_score, sentiment_label)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                            \"\"\"\n",
    "                            \n",
    "                            # Parse published date\n",
    "                            published = article.get('publishedAt', '')\n",
    "                            if published:\n",
    "                                published_dt = datetime.strptime(published, '%Y-%m-%dT%H:%M:%SZ')\n",
    "                            else:\n",
    "                                published_dt = datetime.now()\n",
    "                            \n",
    "                            values = (\n",
    "                                symbol,\n",
    "                                article.get('source', {}).get('name', 'Unknown')[:100],\n",
    "                                article.get('title', '')[:500],\n",
    "                                article.get('description', '')[:1000],\n",
    "                                article.get('url', '')[:500],\n",
    "                                published_dt,\n",
    "                                sentiment_score,\n",
    "                                sentiment_label\n",
    "                            )\n",
    "                            \n",
    "                            cursor.execute(query, values)\n",
    "                            total_articles += 1\n",
    "                            \n",
    "                        except mysql.connector.Error as e:\n",
    "                            if \"Duplicate entry\" not in str(e):\n",
    "                                self.logger.error(f\"Error inserting news article: {e}\")\n",
    "                    \n",
    "                    print(f\"  ✓ Found {len(articles)} articles\")\n",
    "                    \n",
    "                elif response.status_code == 426:\n",
    "                    self.logger.warning(\"NewsAPI rate limit reached. Upgrade plan or wait.\")\n",
    "                    break\n",
    "                else:\n",
    "                    self.logger.error(f\"NewsAPI error for {symbol}: {response.status_code}\")\n",
    "                \n",
    "                # Rate limiting - NewsAPI allows 1 request per second on free tier\n",
    "                time.sleep(1.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error scraping news for {symbol}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        self.logger.info(f\"Scraped {total_articles} news articles\")\n",
    "        return True\n",
    "    \n",
    "    def aggregate_daily_sentiment(self, date=None):\n",
    "        \"\"\"\n",
    "        Aggregate sentiment data by symbol and date\n",
    "        date: specific date to aggregate (default: today)\n",
    "        \"\"\"\n",
    "        if date is None:\n",
    "            date = datetime.now().date()\n",
    "        \n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get list of symbols\n",
    "        cursor.execute(\"SELECT DISTINCT symbol FROM stocks\")\n",
    "        symbols = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            try:\n",
    "                # Aggregate Reddit sentiment\n",
    "                reddit_query = \"\"\"\n",
    "                SELECT \n",
    "                    AVG(sentiment_score) as avg_sentiment,\n",
    "                    COUNT(*) as post_count\n",
    "                FROM reddit_sentiment\n",
    "                WHERE symbol = %s AND DATE(created_utc) = %s\n",
    "                \"\"\"\n",
    "                cursor.execute(reddit_query, (symbol, date))\n",
    "                reddit_result = cursor.fetchone()\n",
    "                reddit_avg = reddit_result[0] if reddit_result[0] else 0\n",
    "                reddit_count = reddit_result[1] if reddit_result[1] else 0\n",
    "                \n",
    "                # Aggregate News sentiment\n",
    "                news_query = \"\"\"\n",
    "                SELECT \n",
    "                    AVG(sentiment_score) as avg_sentiment,\n",
    "                    COUNT(*) as article_count\n",
    "                FROM news_sentiment\n",
    "                WHERE symbol = %s AND DATE(published_at) = %s\n",
    "                \"\"\"\n",
    "                cursor.execute(news_query, (symbol, date))\n",
    "                news_result = cursor.fetchone()\n",
    "                news_avg = news_result[0] if news_result[0] else 0\n",
    "                news_count = news_result[1] if news_result[1] else 0\n",
    "                \n",
    "                # Calculate combined sentiment (weighted by count)\n",
    "                total_count = reddit_count + news_count\n",
    "                if total_count > 0:\n",
    "                    combined = ((reddit_avg * reddit_count) + (news_avg * news_count)) / total_count\n",
    "                else:\n",
    "                    combined = 0\n",
    "                \n",
    "                # Insert or update aggregated data\n",
    "                insert_query = \"\"\"\n",
    "                INSERT INTO daily_sentiment \n",
    "                (symbol, date, reddit_avg_sentiment, reddit_post_count, \n",
    "                 news_avg_sentiment, news_article_count, combined_sentiment, total_mentions)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON DUPLICATE KEY UPDATE\n",
    "                    reddit_avg_sentiment = VALUES(reddit_avg_sentiment),\n",
    "                    reddit_post_count = VALUES(reddit_post_count),\n",
    "                    news_avg_sentiment = VALUES(news_avg_sentiment),\n",
    "                    news_article_count = VALUES(news_article_count),\n",
    "                    combined_sentiment = VALUES(combined_sentiment),\n",
    "                    total_mentions = VALUES(total_mentions)\n",
    "                \"\"\"\n",
    "                \n",
    "                cursor.execute(insert_query, (\n",
    "                    symbol, date, reddit_avg, reddit_count,\n",
    "                    news_avg, news_count, combined, total_count\n",
    "                ))\n",
    "                \n",
    "            except mysql.connector.Error as e:\n",
    "                self.logger.error(f\"Error aggregating sentiment for {symbol}: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        self.logger.info(f\"Aggregated sentiment data for {len(symbols)} symbols on {date}\")\n",
    "        return True\n",
    "    \n",
    "    def get_stock_symbols(self):\n",
    "        \"\"\"Get all tracked stock symbols from database\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return []\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT symbol FROM stocks\")\n",
    "        symbols = [row[0] for row in cursor.fetchall()]\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_config = {\n",
    "    'client_id': '6SD-D4KilOopu4O6m9R9VA',\n",
    "    'client_secret': 'mv-Z3_vFTC7WApSWvtdxUX0MKDq3QQ',\n",
    "    'user_agent': 'Trading Sentiment Bot by Delicious_Divide6891'\n",
    "}\n",
    "\n",
    "news_api_key = '73e9447f080543c3885ec7803f705101'\n",
    "\n",
    "db_config = {\n",
    "    'host': '127.0.0.1',\n",
    "    'user': 'root',\n",
    "    'password': '',\n",
    "    'database': 'trading_system'\n",
    "}\n",
    "\n",
    "# Initialize scraper\n",
    "scraper = SentimentScraper(reddit_config, news_api_key, db_config)\n",
    "\n",
    "# Create sentiment tables\n",
    "scraper.create_sentiment_tables()\n",
    "\n",
    "# Get stock symbols from database\n",
    "symbols = scraper.get_stock_symbols()\n",
    "print(f\"✓ Setup complete! Tracking {len(symbols)} stocks: {symbols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Reddit scraping...\")\n",
    "scraper.scrape_reddit(symbols, limit=50, time_filter='week')\n",
    "print(\"\\n✓ Reddit scraping complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting news scraping...\")\n",
    "print(\"This will take 1-2 minutes.\\n\")\n",
    "\n",
    "scraper.scrape_news(symbols, days_back=7)\n",
    "\n",
    "print(\"\\n✓ News scraping complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49267b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggregating daily sentiment scores...\")\n",
    "\n",
    "scraper.aggregate_daily_sentiment()\n",
    "\n",
    "print(\"✓ Sentiment aggregation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab25017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "\n",
    "# Reddit posts count\n",
    "reddit_df = pd.read_sql(\"SELECT COUNT(*) as total FROM reddit_sentiment\", conn)\n",
    "print(f\"Reddit posts collected: {reddit_df['total'][0]}\")\n",
    "\n",
    "# News articles count\n",
    "news_df = pd.read_sql(\"SELECT COUNT(*) as total FROM news_sentiment\", conn)\n",
    "print(f\"News articles collected: {news_df['total'][0]}\")\n",
    "\n",
    "# Top stocks by sentiment mentions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 STOCKS BY TOTAL MENTIONS:\")\n",
    "print(\"=\"*60)\n",
    "sentiment_df = pd.read_sql(\"\"\"\n",
    "    SELECT symbol, combined_sentiment, total_mentions, \n",
    "           reddit_post_count, news_article_count\n",
    "    FROM daily_sentiment \n",
    "    WHERE date = CURDATE()\n",
    "    ORDER BY total_mentions DESC \n",
    "    LIMIT 10\n",
    "\"\"\", conn)\n",
    "\n",
    "for idx, row in sentiment_df.iterrows():\n",
    "    print(f\"{row['symbol']:6s} | Sentiment: {row['combined_sentiment']:+.3f} | \"\n",
    "          f\"Mentions: {row['total_mentions']:3d} \"\n",
    "          f\"(Reddit: {row['reddit_post_count']}, News: {row['news_article_count']})\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n✓ All sentiment data collection complete!\")\n",
    "print(\"Check your database tables: reddit_sentiment, news_sentiment, daily_sentiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
