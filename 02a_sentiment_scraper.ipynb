{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b57e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import requests\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import logging\n",
    "from textblob import TextBlob\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import re\n",
    "\n",
    "print(\"✓ Imports loaded\")\n",
    "print(\"✓ Loading FinBERT model... (this takes 30 seconds first time)\")\n",
    "\n",
    "# Initialize FinBERT\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "finbert_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "finbert_model.eval()\n",
    "\n",
    "print(\"✓ FinBERT model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66414e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_textblob(text):\n",
    "    \"\"\"TextBlob sentiment analysis (original method)\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return 0.0, 'neutral'\n",
    "    \n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity  # -1 to +1\n",
    "    \n",
    "    if polarity > 0.1:\n",
    "        label = 'positive'\n",
    "    elif polarity < -0.1:\n",
    "        label = 'negative'\n",
    "    else:\n",
    "        label = 'neutral'\n",
    "    \n",
    "    return float(polarity), label\n",
    "\n",
    "\n",
    "def analyze_sentiment_finbert(text):\n",
    "    \"\"\"FinBERT sentiment analysis (financial-specific)\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return 0.0, 'neutral'\n",
    "    \n",
    "    # Truncate to 512 tokens (BERT limit)\n",
    "    inputs = finbert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = finbert_model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # FinBERT outputs: [positive, negative, neutral]\n",
    "    positive = predictions[0][0].item()\n",
    "    negative = predictions[0][1].item()\n",
    "    neutral = predictions[0][2].item()\n",
    "    \n",
    "    # Calculate sentiment score (-1 to +1)\n",
    "    sentiment_score = positive - negative\n",
    "    \n",
    "    # Determine label\n",
    "    max_score = max(positive, negative, neutral)\n",
    "    if max_score == positive:\n",
    "        sentiment_label = 'positive'\n",
    "    elif max_score == negative:\n",
    "        sentiment_label = 'negative'\n",
    "    else:\n",
    "        sentiment_label = 'neutral'\n",
    "    \n",
    "    return float(sentiment_score), sentiment_label\n",
    "\n",
    "\n",
    "# Default to FinBERT (better for financial text)\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Main sentiment function - uses FinBERT\"\"\"\n",
    "    return analyze_sentiment_finbert(text)\n",
    "\n",
    "\n",
    "print(\"✓ Sentiment functions ready (using FinBERT by default)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32681d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "reddit_config = {\n",
    "    'client_id': '6SD-D4KilOopu4O6m9R9VA',\n",
    "    'client_secret': 'mv-Z3_vFTC7WApSWvtdxUX0MKDq3QQ',\n",
    "    'user_agent': 'Trading Sentiment Bot by Delicious_Divide6891'\n",
    "}\n",
    "\n",
    "news_api_key = '73e9447f080543c3885ec7803f705101'\n",
    "\n",
    "db_config = {\n",
    "    'host': '127.0.0.1',\n",
    "    'user': 'root',\n",
    "    'password': '',\n",
    "    'database': 'trading_system'\n",
    "}\n",
    "\n",
    "print(\"✓ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentScraper:\n",
    "    def __init__(self, reddit_config, news_api_key, db_config):\n",
    "        self.reddit_config = reddit_config\n",
    "        self.news_api_key = news_api_key\n",
    "        self.db_config = db_config\n",
    "        self.reddit = None\n",
    "        \n",
    "    def connect_db(self):\n",
    "        try:\n",
    "            return mysql.connector.connect(**self.db_config)\n",
    "        except mysql.connector.Error as e:\n",
    "            print(f\"Database error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def init_reddit(self):\n",
    "        try:\n",
    "            self.reddit = praw.Reddit(\n",
    "                client_id=self.reddit_config['client_id'],\n",
    "                client_secret=self.reddit_config['client_secret'],\n",
    "                user_agent=self.reddit_config['user_agent']\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Reddit API error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scrape_reddit(self, symbols, subreddits=None, limit=100, time_filter='month'):\n",
    "        \"\"\"Scrape Reddit with BOTH sentiment analyzers\"\"\"\n",
    "        if subreddits is None:\n",
    "            subreddits = [\n",
    "                'stocks', 'investing', 'wallstreetbets', 'SecurityAnalysis',\n",
    "                'StockMarket', 'options', 'Daytrading', 'ValueInvesting',\n",
    "                'pennystocks', 'FinancialMarkets', 'AlgoTrading', 'dividends',\n",
    "                'RobinHood', 'Bogleheads', 'ETFs', 'technicalanalysis'\n",
    "            ]\n",
    "        \n",
    "        if not self.reddit:\n",
    "            if not self.init_reddit():\n",
    "                return False\n",
    "        \n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        total_posts = 0\n",
    "        \n",
    "        for idx, symbol in enumerate(symbols, 1):\n",
    "            print(f\"\\n[{idx}/{len(symbols)}] Processing {symbol}...\")\n",
    "            for subreddit_name in subreddits:\n",
    "                try:\n",
    "                    print(f\"  → Searching r/{subreddit_name}...\", end='')\n",
    "                    subreddit = self.reddit.subreddit(subreddit_name)\n",
    "                    search_query = f\"${symbol} OR {symbol}\"\n",
    "                    \n",
    "                    post_count = 0\n",
    "                    for post in subreddit.search(search_query, time_filter=time_filter, limit=limit):\n",
    "                        post_count += 1\n",
    "                        full_text = f\"{post.title} {post.selftext}\"\n",
    "                        \n",
    "                        # Get BOTH sentiment scores\n",
    "                        textblob_score, _ = analyze_sentiment_textblob(full_text)\n",
    "                        finbert_score, finbert_label = analyze_sentiment_finbert(full_text)\n",
    "                        \n",
    "                        try:\n",
    "                            query = \"\"\"\n",
    "                            INSERT IGNORE INTO reddit_sentiment \n",
    "                            (symbol, subreddit, post_id, title, selftext, score, num_comments, \n",
    "                             created_utc, sentiment_textblob, sentiment_finbert, sentiment_label)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                            \"\"\"\n",
    "                            \n",
    "                            values = (\n",
    "                                symbol, subreddit_name, post.id,\n",
    "                                post.title[:500], post.selftext[:1000] if post.selftext else '',\n",
    "                                post.score, post.num_comments,\n",
    "                                datetime.fromtimestamp(post.created_utc),\n",
    "                                textblob_score, finbert_score, finbert_label\n",
    "                            )\n",
    "                            \n",
    "                            cursor.execute(query, values)\n",
    "                            total_posts += 1\n",
    "                            \n",
    "                        except mysql.connector.Error as e:\n",
    "                            if \"Duplicate entry\" not in str(e):\n",
    "                                print(f\"Error: {e}\")\n",
    "                    \n",
    "                    print(f\" {post_count} posts found\")\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error scraping r/{subreddit_name}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"\\n✓ Scraped {total_posts} Reddit posts\")\n",
    "        return True\n",
    "    \n",
    "    def scrape_news(self, symbols, days_back=30):\n",
    "        \"\"\"Scrape news with BOTH sentiment analyzers\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        total_articles = 0\n",
    "        \n",
    "        to_date = datetime.now()\n",
    "        from_date = to_date - timedelta(days=days_back)\n",
    "        \n",
    "        for idx, symbol in enumerate(symbols, 1):\n",
    "            print(f\"\\n[{idx}/{len(symbols)}] Fetching news for {symbol}...\")\n",
    "            try:\n",
    "                url = 'https://newsapi.org/v2/everything'\n",
    "                params = {\n",
    "                    'q': f\"{symbol} stock OR {symbol} shares\",\n",
    "                    'from': from_date.strftime('%Y-%m-%d'),\n",
    "                    'to': to_date.strftime('%Y-%m-%d'),\n",
    "                    'language': 'en',\n",
    "                    'sortBy': 'publishedAt',\n",
    "                    'apiKey': self.news_api_key,\n",
    "                    'pageSize': 100\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, params=params)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    articles = data.get('articles', [])\n",
    "                    \n",
    "                    for article in articles:\n",
    "                        text = f\"{article.get('title', '')} {article.get('description', '')}\"\n",
    "                        \n",
    "                        # Get BOTH sentiment scores\n",
    "                        textblob_score, _ = analyze_sentiment_textblob(text)\n",
    "                        finbert_score, finbert_label = analyze_sentiment_finbert(text)\n",
    "                        \n",
    "                        try:\n",
    "                            query = \"\"\"\n",
    "                            INSERT IGNORE INTO news_sentiment\n",
    "                            (symbol, source, title, description, published_at, \n",
    "                             sentiment_textblob, sentiment_finbert, sentiment_label)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                            \"\"\"\n",
    "                            \n",
    "                            published = article.get('publishedAt', '')\n",
    "                            if published:\n",
    "                                published_dt = datetime.strptime(published, '%Y-%m-%dT%H:%M:%SZ')\n",
    "                            else:\n",
    "                                published_dt = datetime.now()\n",
    "                            \n",
    "                            values = (\n",
    "                                symbol,\n",
    "                                article.get('source', {}).get('name', 'Unknown')[:100],\n",
    "                                article.get('title', '')[:500],\n",
    "                                article.get('description', '')[:1000],\n",
    "                                published_dt,\n",
    "                                textblob_score, finbert_score, finbert_label\n",
    "                            )\n",
    "                            \n",
    "                            cursor.execute(query, values)\n",
    "                            total_articles += 1\n",
    "                            \n",
    "                        except mysql.connector.Error as e:\n",
    "                            if \"Duplicate entry\" not in str(e):\n",
    "                                print(f\"Error: {e}\")\n",
    "                    \n",
    "                    print(f\"  ✓ Found {len(articles)} articles\")\n",
    "                    \n",
    "                elif response.status_code == 426:\n",
    "                    print(\"⚠️ NewsAPI rate limit reached\")\n",
    "                    break\n",
    "                \n",
    "                time.sleep(1.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping news for {symbol}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"\\n✓ Scraped {total_articles} news articles\")\n",
    "        return True\n",
    "    \n",
    "    def aggregate_daily_sentiment(self, date=None):\n",
    "        \"\"\"Aggregate BOTH sentiment types by date\"\"\"\n",
    "        if date is None:\n",
    "            date = datetime.now().date()\n",
    "        \n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return False\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"SELECT DISTINCT symbol FROM stocks WHERE is_active = 1\")\n",
    "        symbols = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            try:\n",
    "                # Reddit sentiment (both types)\n",
    "                reddit_query = \"\"\"\n",
    "                SELECT \n",
    "                    AVG(sentiment_textblob) as avg_textblob,\n",
    "                    AVG(sentiment_finbert) as avg_finbert,\n",
    "                    COUNT(*) as post_count\n",
    "                FROM reddit_sentiment\n",
    "                WHERE symbol = %s AND DATE(created_utc) = %s\n",
    "                \"\"\"\n",
    "                cursor.execute(reddit_query, (symbol, date))\n",
    "                reddit_result = cursor.fetchone()\n",
    "                reddit_textblob = reddit_result[0] if reddit_result[0] else 0\n",
    "                reddit_finbert = reddit_result[1] if reddit_result[1] else 0\n",
    "                reddit_count = reddit_result[2] if reddit_result[2] else 0\n",
    "                \n",
    "                # News sentiment (both types)\n",
    "                news_query = \"\"\"\n",
    "                SELECT \n",
    "                    AVG(sentiment_textblob) as avg_textblob,\n",
    "                    AVG(sentiment_finbert) as avg_finbert,\n",
    "                    COUNT(*) as article_count\n",
    "                FROM news_sentiment\n",
    "                WHERE symbol = %s AND DATE(published_at) = %s\n",
    "                \"\"\"\n",
    "                cursor.execute(news_query, (symbol, date))\n",
    "                news_result = cursor.fetchone()\n",
    "                news_textblob = news_result[0] if news_result[0] else 0\n",
    "                news_finbert = news_result[1] if news_result[1] else 0\n",
    "                news_count = news_result[2] if news_result[2] else 0\n",
    "                \n",
    "                # Combined sentiment (weighted average)\n",
    "                total_count = reddit_count + news_count\n",
    "                if total_count > 0:\n",
    "                    combined_textblob = ((reddit_textblob * reddit_count) + \n",
    "                                        (news_textblob * news_count)) / total_count\n",
    "                    combined_finbert = ((reddit_finbert * reddit_count) + \n",
    "                                       (news_finbert * news_count)) / total_count\n",
    "                else:\n",
    "                    combined_textblob = 0\n",
    "                    combined_finbert = 0\n",
    "                \n",
    "                # Insert/update\n",
    "                insert_query = \"\"\"\n",
    "                INSERT INTO daily_sentiment \n",
    "                (symbol, date, reddit_avg_textblob, reddit_avg_finbert, reddit_post_count, \n",
    "                 news_avg_textblob, news_avg_finbert, news_article_count, \n",
    "                 combined_textblob, combined_finbert, total_mentions)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON DUPLICATE KEY UPDATE\n",
    "                    reddit_avg_textblob = VALUES(reddit_avg_textblob),\n",
    "                    reddit_avg_finbert = VALUES(reddit_avg_finbert),\n",
    "                    reddit_post_count = VALUES(reddit_post_count),\n",
    "                    news_avg_textblob = VALUES(news_avg_textblob),\n",
    "                    news_avg_finbert = VALUES(news_avg_finbert),\n",
    "                    news_article_count = VALUES(news_article_count),\n",
    "                    combined_textblob = VALUES(combined_textblob),\n",
    "                    combined_finbert = VALUES(combined_finbert),\n",
    "                    total_mentions = VALUES(total_mentions)\n",
    "                \"\"\"\n",
    "                \n",
    "                cursor.execute(insert_query, (\n",
    "                    symbol, date, reddit_textblob, reddit_finbert, reddit_count,\n",
    "                    news_textblob, news_finbert, news_count,\n",
    "                    combined_textblob, combined_finbert, total_count\n",
    "                ))\n",
    "                \n",
    "            except mysql.connector.Error as e:\n",
    "                print(f\"Error aggregating for {symbol}: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"✓ Aggregated sentiment for {len(symbols)} symbols on {date}\")\n",
    "        return True\n",
    "    \n",
    "    def get_stock_symbols(self):\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return []\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT symbol FROM stocks WHERE is_active = 1\")\n",
    "        symbols = [row[0] for row in cursor.fetchall()]\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentScraper:\n",
    "    def __init__(self, reddit_config, news_api_key, db_config):\n",
    "        self.reddit_config = reddit_config\n",
    "        self.news_api_key = news_api_key\n",
    "        self.db_config = db_config\n",
    "        self.reddit = None\n",
    "        \n",
    "    def connect_db(self):\n",
    "        try:\n",
    "            return mysql.connector.connect(**self.db_config)\n",
    "        except mysql.connector.Error as e:\n",
    "            print(f\"Database error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def init_reddit(self):\n",
    "        try:\n",
    "            self.reddit = praw.Reddit(\n",
    "                client_id=self.reddit_config['client_id'],\n",
    "                client_secret=self.reddit_config['client_secret'],\n",
    "                user_agent=self.reddit_config['user_agent']\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Reddit API error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # --- New/Fixed Method for Table Creation ---\n",
    "    def create_sentiment_tables(self):\n",
    "        \"\"\"Create tables that store BOTH TextBlob and FinBERT sentiment\"\"\"\n",
    "        conn = self.connect_db() # Use self.connect_db() for consistency\n",
    "        \n",
    "        if not conn:\n",
    "            print(\"Could not connect to database for table creation.\")\n",
    "            return\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Drop and recreate tables to add new columns\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS reddit_sentiment\")\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS news_sentiment\") \n",
    "        cursor.execute(\"DROP TABLE IF EXISTS daily_sentiment\")\n",
    "        \n",
    "        # Reddit sentiment - WITH BOTH TEXTBLOB AND FINBERT\n",
    "        reddit_table = \"\"\"\n",
    "        CREATE TABLE reddit_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            symbol VARCHAR(10),\n",
    "            subreddit VARCHAR(50),\n",
    "            post_id VARCHAR(20) UNIQUE,\n",
    "            title TEXT,\n",
    "            selftext TEXT,\n",
    "            score INT,\n",
    "            num_comments INT,\n",
    "            created_utc TIMESTAMP,\n",
    "            sentiment_textblob DECIMAL(5,4),\n",
    "            sentiment_finbert DECIMAL(5,4),\n",
    "            sentiment_label VARCHAR(20),\n",
    "            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            INDEX idx_symbol (symbol),\n",
    "            INDEX idx_created (created_utc)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # News sentiment - WITH BOTH TEXTBLOB AND FINBERT\n",
    "        news_table = \"\"\"\n",
    "        CREATE TABLE news_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            symbol VARCHAR(10),\n",
    "            source VARCHAR(100),\n",
    "            title TEXT,\n",
    "            description TEXT,\n",
    "            published_at TIMESTAMP,\n",
    "            sentiment_textblob DECIMAL(5,4),\n",
    "            sentiment_finbert DECIMAL(5,4),\n",
    "            sentiment_label VARCHAR(20),\n",
    "            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            INDEX idx_symbol (symbol),\n",
    "            INDEX idx_published (published_at)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # Daily sentiment - WITH BOTH METHODS\n",
    "        daily_sentiment_table = \"\"\"\n",
    "        CREATE TABLE daily_sentiment (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            symbol VARCHAR(10),\n",
    "            date DATE,\n",
    "            reddit_avg_textblob DECIMAL(5,4),\n",
    "            reddit_avg_finbert DECIMAL(5,4),\n",
    "            reddit_post_count INT DEFAULT 0,\n",
    "            news_avg_textblob DECIMAL(5,4),\n",
    "            news_avg_finbert DECIMAL(5,4),\n",
    "            news_article_count INT DEFAULT 0,\n",
    "            combined_textblob DECIMAL(5,4),\n",
    "            combined_finbert DECIMAL(5,4),\n",
    "            total_mentions INT DEFAULT 0,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE KEY unique_symbol_date (symbol, date)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(reddit_table)\n",
    "        cursor.execute(news_table)\n",
    "        cursor.execute(daily_sentiment_table)\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"✓ Enhanced sentiment tables created (with TextBlob + FinBERT)\")\n",
    "    \n",
    "    # --- Existing scraping and aggregation methods follow... ---\n",
    "    def scrape_reddit(self, symbols, subreddits=None, limit=100, time_filter='month'):\n",
    "        # ... (rest of the scrape_reddit method)\n",
    "        pass # Placeholder for brevity, the original content is correct\n",
    "    \n",
    "    def scrape_news(self, symbols, days_back=30):\n",
    "        # ... (rest of the scrape_news method)\n",
    "        pass # Placeholder for brevity, the original content is correct\n",
    "\n",
    "    def aggregate_daily_sentiment(self, date=None):\n",
    "        # ... (rest of the aggregate_daily_sentiment method)\n",
    "        pass # Placeholder for brevity, the original content is correct\n",
    "    \n",
    "    def get_stock_symbols(self):\n",
    "        conn = self.connect_db()\n",
    "        if not conn:\n",
    "            return []\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT symbol FROM stocks WHERE is_active = 1\")\n",
    "        symbols = [row[0] for row in cursor.fetchall()]\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return symbols\n",
    "\n",
    "\n",
    "print(\"✓ Enhanced SentimentScraper class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ec12f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 00:26:36,722 - INFO - package: mysql.connector.plugins\n",
      "2025-10-23 00:26:36,724 - INFO - plugin_name: caching_sha2_password\n",
      "2025-10-23 00:26:36,725 - INFO - AUTHENTICATION_PLUGIN_CLASS: MySQLCachingSHA2PasswordAuthPlugin\n",
      "2025-10-23 00:26:36,756 - INFO - Sentiment tables created successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete! Tracking 24 stocks: ['AAPL', 'AMD', 'AMZN', 'BLK', 'CHN', 'ERO', 'FXP', 'GOOGL', 'GXC', 'JPM', 'KR', 'MDT', 'META', 'MSFT', 'NFLX', 'NVDA', 'OXY', 'PGJ', 'RSP', 'SPY', 'TSLA', 'VGK', 'XPP', 'YINN']\n"
     ]
    }
   ],
   "source": [
    "# Initialize scraper\n",
    "scraper = SentimentScraper(reddit_config, news_api_key, db_config)\n",
    "\n",
    "# Create sentiment tables\n",
    "scraper.create_sentiment_tables()\n",
    "\n",
    "# Get stock symbols from database\n",
    "symbols = scraper.get_stock_symbols()\n",
    "print(f\"✓ Setup complete! Tracking {len(symbols)} stocks: {symbols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128f8a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 00:26:40,031 - INFO - Reddit API initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Reddit scraping...\n",
      "\n",
      "[1/24] Processing AAPL...\n",
      "  → Searching r/stocks... 3 posts found\n",
      "  → Searching r/investing... 1 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[2/24] Processing AMD...\n",
      "  → Searching r/stocks... 2 posts found\n",
      "  → Searching r/investing... 2 posts found\n",
      "  → Searching r/wallstreetbets... 13 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[3/24] Processing AMZN...\n",
      "  → Searching r/stocks... 4 posts found\n",
      "  → Searching r/investing... 2 posts found\n",
      "  → Searching r/wallstreetbets... 3 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[4/24] Processing BLK...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[5/24] Processing CHN...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[6/24] Processing ERO...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[7/24] Processing FXP...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[8/24] Processing GOOGL...\n",
      "  → Searching r/stocks... 18 posts found\n",
      "  → Searching r/investing... 11 posts found\n",
      "  → Searching r/wallstreetbets... 9 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[9/24] Processing GXC...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[10/24] Processing JPM...\n",
      "  → Searching r/stocks... 1 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[11/24] Processing KR...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[12/24] Processing MDT...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[13/24] Processing META...\n",
      "  → Searching r/stocks... 2 posts found\n",
      "  → Searching r/investing... 6 posts found\n",
      "  → Searching r/wallstreetbets... 4 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[14/24] Processing MSFT...\n",
      "  → Searching r/stocks... 2 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[15/24] Processing NFLX...\n",
      "  → Searching r/stocks... 2 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 5 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[16/24] Processing NVDA...\n",
      "  → Searching r/stocks... 4 posts found\n",
      "  → Searching r/investing... 5 posts found\n",
      "  → Searching r/wallstreetbets... 12 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[17/24] Processing OXY...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 1 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[18/24] Processing PGJ...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[19/24] Processing RSP...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 1 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[20/24] Processing SPY...\n",
      "  → Searching r/stocks... 10 posts found\n",
      "  → Searching r/investing... 4 posts found\n",
      "  → Searching r/wallstreetbets... 19 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[21/24] Processing TSLA...\n",
      "  → Searching r/stocks... 3 posts found\n",
      "  → Searching r/investing... 2 posts found\n",
      "  → Searching r/wallstreetbets... 4 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[22/24] Processing VGK...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[23/24] Processing XPP...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n",
      "\n",
      "[24/24] Processing YINN...\n",
      "  → Searching r/stocks... 0 posts found\n",
      "  → Searching r/investing... 0 posts found\n",
      "  → Searching r/wallstreetbets... 0 posts found\n",
      "  → Searching r/SecurityAnalysis... 0 posts found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 00:30:30,224 - INFO - Scraped 155 Reddit posts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Reddit scraping complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Reddit scraping...\")\n",
    "scraper.scrape_reddit(symbols, limit=50, time_filter='week')\n",
    "print(\"\\n✓ Reddit scraping complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4998b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting news scraping...\n",
      "This will take 1-2 minutes.\n",
      "\n",
      "\n",
      "[1/24] Fetching news for AAPL...\n",
      "  ✓ Found 39 articles\n",
      "\n",
      "[2/24] Fetching news for AMD...\n",
      "  ✓ Found 56 articles\n",
      "\n",
      "[3/24] Fetching news for AMZN...\n",
      "  ✓ Found 22 articles\n",
      "\n",
      "[4/24] Fetching news for BLK...\n",
      "  ✓ Found 4 articles\n",
      "\n",
      "[5/24] Fetching news for CHN...\n",
      "  ✓ Found 0 articles\n",
      "\n",
      "[6/24] Fetching news for ERO...\n",
      "  ✓ Found 2 articles\n",
      "\n",
      "[7/24] Fetching news for FXP...\n",
      "  ✓ Found 0 articles\n",
      "\n",
      "[8/24] Fetching news for GOOGL...\n",
      "  ✓ Found 16 articles\n",
      "\n",
      "[9/24] Fetching news for GXC...\n",
      "  ✓ Found 0 articles\n",
      "\n",
      "[10/24] Fetching news for JPM...\n",
      "  ✓ Found 19 articles\n",
      "\n",
      "[11/24] Fetching news for KR...\n",
      "  ✓ Found 4 articles\n",
      "\n",
      "[12/24] Fetching news for MDT...\n",
      "  ✓ Found 1 articles\n",
      "\n",
      "[13/24] Fetching news for META...\n",
      "  ✓ Found 93 articles\n",
      "\n",
      "[14/24] Fetching news for MSFT...\n",
      "  ✓ Found 14 articles\n",
      "\n",
      "[15/24] Fetching news for NFLX...\n",
      "  ✓ Found 25 articles\n",
      "\n",
      "[16/24] Fetching news for NVDA...\n",
      "  ✓ Found 46 articles\n",
      "\n",
      "[17/24] Fetching news for OXY...\n",
      "  ✓ Found 1 articles\n",
      "\n",
      "[18/24] Fetching news for PGJ...\n",
      "  ✓ Found 0 articles\n",
      "\n",
      "[19/24] Fetching news for RSP...\n",
      "  ✓ Found 0 articles\n",
      "\n",
      "[20/24] Fetching news for SPY...\n",
      "  ✓ Found 25 articles\n",
      "\n",
      "[21/24] Fetching news for TSLA...\n",
      "  ✓ Found 29 articles\n",
      "\n",
      "[22/24] Fetching news for VGK...\n",
      "  ✓ Found 1 articles\n",
      "\n",
      "[23/24] Fetching news for XPP...\n",
      "  ✓ Found 0 articles\n",
      "\n",
      "[24/24] Fetching news for YINN...\n",
      "  ✓ Found 0 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 00:45:24,050 - INFO - Scraped 397 news articles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ News scraping complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting news scraping...\")\n",
    "print(\"This will take 1-2 minutes.\\n\")\n",
    "\n",
    "scraper.scrape_news(symbols, days_back=7)\n",
    "\n",
    "print(\"\\n✓ News scraping complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49267b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating daily sentiment scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 00:45:57,308 - INFO - Aggregated sentiment data for 24 symbols on 2025-10-23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sentiment aggregation complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Aggregating daily sentiment scores...\")\n",
    "\n",
    "scraper.aggregate_daily_sentiment()\n",
    "\n",
    "print(\"✓ Sentiment aggregation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab25017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit posts collected: 499\n",
      "News articles collected: 4159\n",
      "\n",
      "============================================================\n",
      "TOP 10 STOCKS BY TOTAL MENTIONS:\n",
      "============================================================\n",
      "GOOGL  | Sentiment: +0.083 | Mentions:   3 (Reddit: 3, News: 0)\n",
      "AMZN   | Sentiment: +0.012 | Mentions:   2 (Reddit: 2, News: 0)\n",
      "SPY    | Sentiment: -0.208 | Mentions:   2 (Reddit: 2, News: 0)\n",
      "NVDA   | Sentiment: +0.135 | Mentions:   1 (Reddit: 1, News: 0)\n",
      "NFLX   | Sentiment: +0.000 | Mentions:   1 (Reddit: 1, News: 0)\n",
      "YINN   | Sentiment: +0.000 | Mentions:   0 (Reddit: 0, News: 0)\n",
      "AMD    | Sentiment: +0.000 | Mentions:   0 (Reddit: 0, News: 0)\n",
      "CHN    | Sentiment: +0.000 | Mentions:   0 (Reddit: 0, News: 0)\n",
      "ERO    | Sentiment: +0.000 | Mentions:   0 (Reddit: 0, News: 0)\n",
      "FXP    | Sentiment: +0.000 | Mentions:   0 (Reddit: 0, News: 0)\n",
      "\n",
      "✓ All sentiment data collection complete!\n",
      "Check your database tables: reddit_sentiment, news_sentiment, daily_sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18kyu\\AppData\\Local\\Temp\\ipykernel_28712\\613416929.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  reddit_df = pd.read_sql(\"SELECT COUNT(*) as total FROM reddit_sentiment\", conn)\n",
      "C:\\Users\\18kyu\\AppData\\Local\\Temp\\ipykernel_28712\\613416929.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  news_df = pd.read_sql(\"SELECT COUNT(*) as total FROM news_sentiment\", conn)\n",
      "C:\\Users\\18kyu\\AppData\\Local\\Temp\\ipykernel_28712\\613416929.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sentiment_df = pd.read_sql(\"\"\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "\n",
    "# Reddit posts count\n",
    "reddit_df = pd.read_sql(\"SELECT COUNT(*) as total FROM reddit_sentiment\", conn)\n",
    "print(f\"Reddit posts collected: {reddit_df['total'][0]}\")\n",
    "\n",
    "# News articles count\n",
    "news_df = pd.read_sql(\"SELECT COUNT(*) as total FROM news_sentiment\", conn)\n",
    "print(f\"News articles collected: {news_df['total'][0]}\")\n",
    "\n",
    "# Top stocks by sentiment mentions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 STOCKS BY TOTAL MENTIONS:\")\n",
    "print(\"=\"*60)\n",
    "sentiment_df = pd.read_sql(\"\"\"\n",
    "    SELECT symbol, combined_finbert AS combined_sentiment, total_mentions, \n",
    "           reddit_post_count, news_article_count\n",
    "    FROM daily_sentiment \n",
    "    WHERE date = CURDATE()\n",
    "    ORDER BY total_mentions DESC \n",
    "    LIMIT 10\n",
    "\"\"\", conn)\n",
    "\n",
    "for idx, row in sentiment_df.iterrows():\n",
    "    print(f\"{row['symbol']:6s} | Sentiment: {row['combined_sentiment']:+.3f} | \"\n",
    "          f\"Mentions: {row['total_mentions']:3d} \"\n",
    "          f\"(Reddit: {row['reddit_post_count']}, News: {row['news_article_count']})\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n✓ All sentiment data collection complete!\")\n",
    "print(\"Check your database tables: reddit_sentiment, news_sentiment, daily_sentiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
